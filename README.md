<img src="images/NN.gif" width="400"/> <img src="images/title.gif" width="400"/> 

<a href="https://colab.research.google.com/github/Nikhil-Kadapala/NeuralNets/blob/main/CNNwithLIME%26Attn.ipynb" target="_blank">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab"/>
</a>
<br/>
![GitHub contributors](https://img.shields.io/github/contributors/Nikhil-Kadapala/NeuralNets)
![GitHub commit activity](https://img.shields.io/github/commit-activity/w/Nikhil-Kadapala/NeuralNets)
</br>
------------------------------------------------------------------------------------------------------------------------------------------------------------------

## Rational Neural Networks (RNNs) ![GitHub License](https://img.shields.io/github/license/Nikhil-Kadapala/NeuralNets)
A project on teaching Deep Neural Networks to Rationalize with LIME Feedback

Most Neural Networks in use today  are black boxes that are hard to interpret due to their complex nature and high dimensionality. 
Interpretability of these Machine Learning Models helps human decision-makers understand how these models rationalize their behavior 
and verify the predictions ultimately building trust and improving safety, especially in high-stakes applications like medicine to diagnose patients. 
Through this study, we show how the black-box nature of these Neural Nets obscures us from the underlying spurious correlations they 
sometimes tend to make between the features that might not align with human reasoning and still manage to deliver predictions with high accuracy. 
This can be a problem when sensitive variables are involved and misclassifications can have catastrophic consequences. 
To that end, we evaluate a classifier designed using a standard Neural Network architecture with LIME.

<img src="images/standard CNN.png" alt="">

<img src="images/custom CNN.png" alt="">
