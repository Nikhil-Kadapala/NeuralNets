{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhil-Kadapala/NeuralNets/blob/main/standardNeuralNets/stdCNN_LIME.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L2Do3b_ytm6"
      },
      "source": [
        "# Final Project CS852 - Foundations of Neural Networks (FALL 2024)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICkcYT3Iytm9"
      },
      "source": [
        "Devin Borchard and Nikhil Kadapala\n",
        "\n",
        "Department of Computer Science, University of New Hampshire\n",
        "\n",
        "ERASER datasets: https://www.eraserbenchmark.com/\n",
        "\n",
        "ERASER paper: https://arxiv.org/pdf/1911.03429\n",
        "\n",
        "LIME paper: https://arxiv.org/pdf/1602.04938"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr8uEiG3ytm9"
      },
      "source": [
        "# Notebook setup and PyTorch Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LJ5b9esmytm-"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "# uncomment one of these versions (depending on whether you are on a computer with a CPU or not)\n",
        "\n",
        "# GPU version\n",
        "# !conda install --yes --prefix {sys.prefix} pytorch torchvision cudatoolkit=10.2 -c pytorch\n",
        "\n",
        "# Just CPU\n",
        "# !conda install --yes --prefix {sys.prefix} pytorch torchvision cpuonly -c pytorch\n",
        "\n",
        "# install `Einops` for einstein-style tensor manipulation in pytorch\n",
        "# Also see https://github.com/arogozhnikov/einops\n",
        "# !conda install --yes --prefix {sys.prefix} einops  -c conda-forge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSKM3pNDytnA",
        "outputId": "042c7f0d-1601-4c4f-fdec-c2b8cd5a9b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3.2553e-02, 5.5087e-01, 8.4201e-02],\n",
            "        [3.3018e-01, 6.2233e-02, 1.9032e-01],\n",
            "        [9.7753e-01, 5.6645e-02, 2.7782e-04],\n",
            "        [9.6492e-01, 4.8524e-01, 3.7122e-01],\n",
            "        [6.2648e-01, 2.7334e-01, 1.2060e-01]])\n",
            "GPU/CUDA available?  True\n",
            "Torch version 2.5.1+cu121\n"
          ]
        }
      ],
      "source": [
        "# torch test\n",
        "import torch\n",
        "x = torch.rand(5, 3)\n",
        "print(x)\n",
        "\n",
        "print(\"GPU/CUDA available? \", torch.cuda.is_available())\n",
        "\n",
        "print(\"Torch version\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIw0tI1IytnB"
      },
      "source": [
        "# **Extracting Traning, Validation, and Test Data**\n",
        "# Parse the data files to extract the reviews, classifications and annotations for each split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIrDTApeytnC"
      },
      "source": [
        "There are three files:\n",
        "- train.jsonl: containts 1600 training examples\n",
        "- val.jsonl: contains 200 validation examples\n",
        "- test.json: contains 199 test examples\n",
        "\n",
        "Each example includes:\n",
        "- annotation_id: a unique id for an example of the form negR_000 for negative examples and posR_000 for positive examples.\n",
        "- evidences: a list of rationales(specific parts of the review) given by humans that most influenced their classification decision.\n",
        "- classification: the class of the example\n",
        "\n",
        "The annotation_id of each example is the name of the file for the input text data\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pVM3qH6FytnD"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def parse_data(file_path):\n",
        "    data = []                                               # Initialize an empty list to store the dictionaries\n",
        "\n",
        "    with open(file_path, 'r') as file:                      # Open the .jsonl file and read it line by line\n",
        "        for line in file:\n",
        "            annotation = json.loads(line)                   # Parse each line as JSON and append it to the list\n",
        "            id = annotation[\"annotation_id\"]\n",
        "            annotation[\"classification\"] = 1 if annotation['classification'] == \"POS\" else 0\n",
        "\n",
        "            with open(f\"./movies/docs/{id}\", 'r') as file:  # open the file named by annotation_id to extract the review text\n",
        "                content = file.read()\n",
        "                annotation['content'] = content.replace('\\n', ' ')\n",
        "                data.append(annotation)\n",
        "    return data\n",
        "\n",
        "# Specify the path to your JSON file\n",
        "train_file_path = './movies/train.jsonl'\n",
        "val_file_path = './movies/val.jsonl'\n",
        "test_file_path = './movies/test.jsonl'\n",
        "\n",
        "train_data = parse_data(train_file_path)\n",
        "validation_data = parse_data(val_file_path)\n",
        "test_data = parse_data(test_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bijy4N-HytnD"
      },
      "source": [
        "# Functions to extract reviews, classifications, and annotations\n",
        "  Define a function to retrieve an example and print the relevant information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hKGZxYzTytnE",
        "outputId": "480c0296-9ee9-4e39-a9aa-e02b2fae817c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset split: 1600 training examples\n",
            "               200 validation examples\n",
            "               199 test examples\n",
            "\n",
            "Retrieving Training Example [506].................\n",
            "\n",
            "Review content:\n",
            "this film is extraordinarily horrendous and i 'm not going to waste any more words on it .\n",
            "\n",
            "---------------------------- \n",
            "| Sentiment class: 0 - NEG | \n",
            "----------------------------\n",
            "\n",
            "Human rationales / Supporting Evidence:\n",
            "     -  extraordinarily horrendous\n"
          ]
        }
      ],
      "source": [
        "def print_example(data, index, print_content=True, print_classification=True, print_rationales=True ):\n",
        "    print(f'Retrieving Training Example [{index}].................\\n')\n",
        "    item = data[index]\n",
        "    classification = item['classification']\n",
        "    evidences = item['evidences']\n",
        "    content = item['content']\n",
        "    if print_content: print(f'Review content:\\n{content}\\n')\n",
        "    if print_classification: print('----------------------------',\n",
        "                                   '\\n| Sentiment class:',\n",
        "                                   classification,\n",
        "                                   (\"- NEG\" if not classification else \"- POS\"),\n",
        "                                   '|', '\\n----------------------------')\n",
        "    if print_rationales:\n",
        "        print('\\nHuman rationales / Supporting Evidence:')\n",
        "        for evidence in evidences:\n",
        "            print('     - ', evidence[0]['text'])\n",
        "\n",
        "def get_content(data, index):\n",
        "    item = data[index]\n",
        "    content = item['content']\n",
        "    return content\n",
        "\n",
        "def get_classes(data, index):\n",
        "    item = data[index]\n",
        "    classification = item['classification']\n",
        "    return classification\n",
        "\n",
        "def get_annotations(data, index):\n",
        "    item = data[index]\n",
        "    content = item['evidences']\n",
        "    annotations = [content[0]['text'] for evidence in content]\n",
        "    return annotations\n",
        "\n",
        "train_size = len(train_data)\n",
        "val_size = len(validation_data)\n",
        "test_size = len(test_data)\n",
        "\n",
        "print(f'Dataset split: {train_size} training examples')\n",
        "print(f'               {val_size} validation examples')\n",
        "print(f'               {test_size} test examples\\n')\n",
        "\n",
        "print_example(train_data, 506)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLQCckZ-wz1C"
      },
      "source": [
        "# Extraction of the rationales from the evidences metadata of each human annotation of reviews.\n",
        "\n",
        "Each annotation of the review is not the highlighted text/rationale itself but also contains metadata of the text. Use the function defined in the above cell to extract just the text and replace the evidences dictionary of the training, validation, and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-5ulEzugwz1C",
        "outputId": "677426bb-c510-48d7-aa02-897c31d4a070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "list indices must be integers or slices, not str",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d84cab35fa62>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'evidences'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'evidences'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-37ee1ac3cba6>\u001b[0m in \u001b[0;36mget_annotations\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'evidences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevidence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-37ee1ac3cba6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'evidences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevidence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
          ]
        }
      ],
      "source": [
        "for i in range(len(train_data)):\n",
        "  train_data[i]['evidences'] = get_annotations(train_data,i)\n",
        "\n",
        "validation_data[i]['evidences'] = [get_annotations(validation_data,i) for i in range(len(validation_data))]\n",
        "\n",
        "test_data[i]['evidences'] = [get_annotations(test_data,i) for i in range(len(test_data))]\n",
        "\n",
        "print(train_data[506]['evidences'])\n",
        "\n",
        "print(validation_data[506]['evidences'])\n",
        "\n",
        "print(test_data[506]['evidences'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc2rC9QTytnK"
      },
      "source": [
        "# Pre-trianed GloVe Embeddings of Training Examples\n",
        "Download the pretrained GloVe Embeddings of desired dimensions using gensim downlader.\n",
        "\n",
        "Save downloaded embeddings to a local file to avoid re-downloading when the kernel or notebook is restarted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWyAJ5D_ytnL"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    Install gensim, to use word2vec word embeddings\n",
        "    Install gensim (for pre-trained word embeddings)\n",
        "    #!conda install --yes --prefix {sys.prefix} gensim\n",
        "\"\"\"\n",
        "#import gensim\n",
        "#import gensim.downloader\n",
        "\n",
        "\"\"\"\n",
        "    ONLY if you get an error after `import gensim`: update your smart_open liberary\n",
        "    #!conda install --yes --prefix {sys.prefix} smart_open\n",
        "    restart your notebook\n",
        "    see if `import gensim` works now\n",
        "\"\"\"\n",
        "#wv = gensim.downloader.load(\"glove-wiki-gigaword-50\")\n",
        "\n",
        "#import pickle\n",
        "\n",
        "#with open(\"glove_embeddings.pkl\", \"wb\") as f:\n",
        "    #pickle.dump(wv, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqA5DXEEwz1D"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"glove_embeddings.pkl\", \"rb\") as f:\n",
        "    wv = pickle.load(f)\n",
        "\n",
        "# lookup the word vector for a word \"india\"\n",
        "wv['india']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFRN-vp2ytnM"
      },
      "outputs": [],
      "source": [
        "# downsampled embedding and zero vector for unknown words\n",
        "# note the following code assums the the word embedding dimensions are dividible by 5\n",
        "\n",
        "import einops # type: ignore\n",
        "import numpy as np\n",
        "from typing import List\n",
        "import types\n",
        "\n",
        "def glove_embed(word:str, target_dim)->np.array:\n",
        "    '''Looks up word in embedding (downsampled to five dimensions), pads with beginning of embedding.\n",
        "       Returns zero vector for unknown words.\n",
        "    '''\n",
        "    # these parameters work for 50-dim glove embeddings (adjust for other embeddings)\n",
        "    sampled_dim = 5\n",
        "    sample_batches = 10\n",
        "\n",
        "    empty_vec=np.zeros(target_dim)\n",
        "    if word in wv:\n",
        "        w2v = wv[word] # lookup 50 dim vector\n",
        "        a=einops.reduce(w2v,'(d seg)-> d', \"sum\", seg=sample_batches)  # downsample\n",
        "        b=w2v[0:target_dim-sampled_dim]\n",
        "        return np.hstack([a,b])\n",
        "    else:\n",
        "        return empty_vec\n",
        "\n",
        "def glove_embed_sequences(sequence, target_dim):\n",
        "\n",
        "    if isinstance(sequence, list):\n",
        "        if len(sequence) == 0:\n",
        "            empty_seq = np.zeros(target_dim)\n",
        "            gloveTensor =  torch.tensor(empty_seq, dtype=torch.float)\n",
        "        else:\n",
        "            tokens = \",\".join(sequence)\n",
        "            words = tokens.split()\n",
        "            gloveTensor = torch.stack([torch.tensor(glove_embed(word, target_dim), dtype=torch.float) for word in words])\n",
        "    else:\n",
        "        tokens = sequence.split()\n",
        "        gloveTensor = torch.stack([torch.tensor(glove_embed(token, target_dim), dtype=torch.float) for token in tokens])\n",
        "\n",
        "    return gloveTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoZiHbVxytnN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "reviews = [get_content(train_data, i) for i in range(train_size)]\n",
        "classes = [get_classes(train_data, i) for i in range(train_size)]\n",
        "rationales = [get_annotations(train_data, i) for i in range(train_size)]\n",
        "\n",
        "print(\"Number of reviews in training data:\",len(reviews))\n",
        "print(\"Max seq length of reviews:\", np.max([len(review.split()) for review in reviews]))\n",
        "\n",
        "df = pd.DataFrame(train_data)\n",
        "df.drop(columns=['query', 'query_type'], inplace=True)\n",
        "df['evidences'] = rationales\n",
        "df.head()\n",
        "#rationales[7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqk66GB4wz1E"
      },
      "outputs": [],
      "source": [
        "# Find the smallest review of all to inspect and understand the training data structure.\n",
        "smallest_entry = df.loc[df['content'].apply(len).idxmin()]\n",
        "print(smallest_entry)\n",
        "id = 'negR_506.txt'\n",
        "index = df['content'].apply(len).idxmin()\n",
        "print(f'{df.iloc[506]['content']} \\n{df.iloc[506]['classification']} \\n{df.iloc[506]['evidences']}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_WbCl45wz1E"
      },
      "source": [
        "Extract validation set from the val.jsonl file and create a dataframe for it similar to the training set and save it to a csv file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60CvIoxMwz1E"
      },
      "outputs": [],
      "source": [
        "# TO DO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbHzxzSlwz1E"
      },
      "source": [
        "# Convert the reviews to their corresponding Glove embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pm21m4EZwz1F"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "reviewGloVes = [glove_embed_sequences(review, 11) for review in df['content']]\n",
        "reviewGloVes = pad_sequence(reviewGloVes, batch_first=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBFXaHfPwz1F"
      },
      "source": [
        "# Convert the rationales to their corresponding Glove embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "et3YJVo8wz1F"
      },
      "outputs": [],
      "source": [
        "\n",
        "rationaleGloVes = [glove_embed_sequences(rationale, 11) for rationale in rationales]\n",
        "rationaleGloVes = pad_sequence(reviewGloVes, batch_first=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSzDGF43ytnN"
      },
      "source": [
        "# Convert the training data to batches using DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsgVwakDytnO"
      },
      "outputs": [],
      "source": [
        "input = reviewGloVes\n",
        "print(\"input dim (batches, max_seq_len, embed_size):\",input.size())\n",
        "Y_star = torch.tensor(classes, dtype=torch.float)\n",
        "train_x = TensorDataset(input, Y_star)\n",
        "dataLoader = DataLoader(train_x, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPQy0BjBytnO"
      },
      "source": [
        "# Convolutional Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smXuMplUytnO"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional, Tuple, Union\n",
        "from torch import Tensor\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: Tuple[int, ...],\n",
        "        pool_size: Tuple[int, ...],\n",
        "        stride: Tuple[int, ...],\n",
        "        padding: Tuple[int, ...],\n",
        "        bias: bool\n",
        "    ) -> None:\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.pool_size = pool_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.bias = bias\n",
        "\n",
        "        super(MyModel, self).__init__()\n",
        "        self.ReLU_Activation = nn.ReLU()\n",
        "        self.convolutionLayer = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
        "        self.poolingLayer = nn.MaxPool1d(kernel_size=pool_size, stride=stride, padding=padding)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        X: Tensor,\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\n",
        "        Embedding Layer X --Theta--> E\n",
        "        Input: X (Tensor)\n",
        "        Output: E (Tensor)\n",
        "        Parameter: Theta (optional)\n",
        "        \"\"\"\n",
        "\n",
        "        X = X.squeeze(0)\n",
        "        X_shape = list(X.size())\n",
        "        #print(X_shape)\n",
        "        #print(X)\n",
        "        self.embeddingLayer = nn.Linear(2809, self.in_channels, bias=False)\n",
        "        self.embeddingLayer.weight.data = torch.randn((3,11),dtype=torch.float)\n",
        "        E = self.embeddingLayer(X)\n",
        "        #print(E)\n",
        "\n",
        "        E_shape = list(E.size())\n",
        "        if len(E_shape) < 3:\n",
        "            E = E.unsqueeze(0)\n",
        "        E = E.permute(0, 2, 1)\n",
        "\n",
        "        \"\"\"\n",
        "        Convolution Layer E --W--> H\n",
        "        Input: E (Tensor)\n",
        "        Output: H (Tensor)\n",
        "        Parameter: W (optional)\n",
        "        \"\"\"\n",
        "        #self.convolutionLayer.weight.data = W_torch.permute(2, 0, 1)\n",
        "        H = self.convolutionLayer(E)\n",
        "        #print(H)\n",
        "        H = H.squeeze(0)\n",
        "        H = H.permute(1, 0)\n",
        "\n",
        "        \"\"\"\n",
        "        Dectector Layer H --Psi--> D with ReLU\n",
        "        Input: H (Tensor)\n",
        "        Output: D (Tensor)\n",
        "        Parameter: Psi (optional)\n",
        "        \"\"\"\n",
        "        myPsi_torch = torch.randn((2,1), dtype=torch.float)\n",
        "        D = torch.einsum('ij,jk->ik', H, myPsi_torch)\n",
        "        self.ReLU_Activation = nn.ReLU()\n",
        "        D = self.ReLU_Activation(D)\n",
        "        #print(D)\n",
        "        D = D.permute(1, 0)\n",
        "\n",
        "        \"\"\"\n",
        "        Pooling Layer D --MaxPool--> Y\n",
        "        Input: D (Tensor)\n",
        "        output: Y (Tensor)\n",
        "        Parameter: Pooling_window (optional)\n",
        "        \"\"\"\n",
        "        P = self.poolingLayer(D)\n",
        "        Y_hat = torch.zeros(X_shape[0])\n",
        "        D_shape = list(D.size())\n",
        "        out_size =  D_shape[1] - self.pool_size // self.stride + 1\n",
        "        Y_hat[:out_size] = P\n",
        "\n",
        "        return Y_hat\n",
        "\n",
        "model = MyModel(3, 2, 3, 2, 1, 0, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOONmTg-ytnP"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IXthmm0ytnP"
      },
      "outputs": [],
      "source": [
        "def train(xdata, ydata):\n",
        "    '''Train the neural model with the given training data'''\n",
        "\n",
        "    #Construct the loss function\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    # Construct the optimizer (Stochastic Gradient Descent in this case)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)  # lr is learning rate\n",
        "\n",
        "    # Gradient Descent\n",
        "    for epoch in range(201):\n",
        "        # Forward pass: Compute predicted y by passing x to the model\n",
        "        Y_pred = model(xdata)\n",
        "\n",
        "        # Compute and print loss\n",
        "        loss = criterion(Y_pred, ydata)\n",
        "\n",
        "        if epoch >0 and epoch % 40 == 0:\n",
        "            print('epoch: ', epoch,' loss: ', loss.item())\n",
        "\n",
        "        # Zero gradients, perform a backward pass, and update the weights.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # perform a backward pass (backpropagation)\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "\n",
        "    return Y_pred\n",
        "\n",
        "batch_input = torch.zeros((2809, 11), dtype=torch.float)\n",
        "predList = []\n",
        "batch_outputs = torch.zeros(5, dtype=torch.float)\n",
        "k = 0\n",
        "for batch in dataLoader:\n",
        "    batch_input = batch[0]\n",
        "    y_star = Y_star[k]\n",
        "    k += 1\n",
        "    print(\"-----------------------------------------------------------------------\")\n",
        "    print(\"Training batch:\", k)\n",
        "    print(\"-----------------------------------------------------------------------\")\n",
        "    Y_pred = train(batch_input, y_star)\n",
        "    #print(Y_pred.size())\n",
        "    predList.append(Y_pred[:5])\n",
        "    if k == 5:\n",
        "        break\n",
        "batch_outputs = torch.stack(predList)\n",
        "#print(batch_outputs.size())\n",
        "print(\"======================================================================\")\n",
        "mse=torch.mean( (batch_outputs-Y_star[:5])**2)  # detach takes the tensor out of the network\n",
        "print(\"MSE for ground truth y_star\", mse)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}