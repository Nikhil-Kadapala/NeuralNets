{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Nikhil-Kadapala/NeuralNets/blob/main/standardNeuralNets/stdCNN_LIME.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6L2Do3b_ytm6"
   },
   "source": [
    "# Final Project CS852 - Foundations of Neural Networks (FALL 2024)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICkcYT3Iytm9"
   },
   "source": [
    "Devin Borchard and Nikhil Kadapala\n",
    "\n",
    "Department of Computer Science, University of New Hampshire\n",
    "\n",
    "ERASER datasets: https://www.eraserbenchmark.com/\n",
    "\n",
    "ERASER paper: https://arxiv.org/pdf/1911.03429\n",
    "\n",
    "LIME paper: https://arxiv.org/pdf/1602.04938"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sr8uEiG3ytm9"
   },
   "source": [
    "# Notebook setup and PyTorch Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LJ5b9esmytm-"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# uncomment one of these versions (depending on whether you are on a computer with a CPU or not)\n",
    "\n",
    "# GPU version\n",
    "# !conda install --yes --prefix {sys.prefix} pytorch torchvision cudatoolkit=10.2 -c pytorch\n",
    "\n",
    "# Just CPU\n",
    "# !conda install --yes --prefix {sys.prefix} pytorch torchvision cpuonly -c pytorch\n",
    "\n",
    "# install `Einops` for einstein-style tensor manipulation in pytorch\n",
    "# Also see https://github.com/arogozhnikov/einops\n",
    "# !conda install --yes --prefix {sys.prefix} einops  -c conda-forge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XSKM3pNDytnA",
    "outputId": "7138dbf0-2936-49a4-b49e-a0b5d580ff9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5923, 0.3155, 0.8683],\n",
      "        [0.4144, 0.0184, 0.1204],\n",
      "        [0.5909, 0.6192, 0.3149],\n",
      "        [0.6107, 0.9267, 0.0519],\n",
      "        [0.7694, 0.9353, 0.5815]])\n",
      "GPU/CUDA available?  False\n",
      "Torch version 2.5.1\n"
     ]
    }
   ],
   "source": [
    "# torch test\n",
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "\n",
    "print(\"GPU/CUDA available? \", torch.cuda.is_available())\n",
    "\n",
    "print(\"Torch version\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIw0tI1IytnB"
   },
   "source": [
    "# **Extracting Traning, Validation, and Test Data**\n",
    "# Parse the data files to extract the reviews, classifications and annotations for each split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIrDTApeytnC"
   },
   "source": [
    "There are three files:\n",
    "- train.jsonl: containts 1600 training examples\n",
    "- val.jsonl: contains 200 validation examples\n",
    "- test.json: contains 199 test examples\n",
    "\n",
    "Each example includes:\n",
    "- annotation_id: a unique id for an example of the form negR_000 for negative examples and posR_000 for positive examples.\n",
    "- evidences: a list of rationales(specific parts of the review) given by humans that most influenced their classification decision.\n",
    "- classification: the class of the example\n",
    "\n",
    "The annotation_id of each example is the name of the file for the input text data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "pVM3qH6FytnD",
    "outputId": "f003ef8e-9081-43b4-8b87-6a02d0501d1a"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_data(file_path):\n",
    "    data = []                                               # Initialize an empty list to store the dictionaries\n",
    "\n",
    "    with open(file_path, 'r') as file:                      # Open the .jsonl file and read it line by line\n",
    "        for line in file:\n",
    "            annotation = json.loads(line)                   # Parse each line as JSON and append it to the list\n",
    "            id = annotation[\"annotation_id\"]\n",
    "            annotation[\"classification\"] = 1 if annotation['classification'] == \"POS\" else 0\n",
    "\n",
    "            with open(f\"./movies/docs/{id}\", 'r') as file:  # open the file named by annotation_id to extract the review text\n",
    "                content = file.read()\n",
    "                annotation['content'] = content.replace('\\n', ' ')\n",
    "                data.append(annotation)\n",
    "    return data\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "train_file_path = './movies/train.jsonl'\n",
    "val_file_path = './movies/val.jsonl'\n",
    "test_file_path = './movies/test.jsonl'\n",
    "\n",
    "train_data = parse_data(train_file_path)\n",
    "validation_data = parse_data(val_file_path)\n",
    "test_data = parse_data(test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bijy4N-HytnD"
   },
   "source": [
    "# Functions to extract reviews, classifications, and annotations\n",
    "  Define a function\n",
    "\n",
    "  i) to retrieve an example and print the relevant information.\n",
    "\n",
    "  ii) to retrieve the content of the example review text\n",
    "\n",
    "  iii) to retrieve the classifications of the examples\n",
    "  \n",
    "  iv) to retrieve the annotations provided to support the classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hKGZxYzTytnE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: 1600 training examples\n",
      "               200 validation examples\n",
      "               199 test examples\n",
      "\n",
      "Retrieving Training Example [506].................\n",
      "\n",
      "Review content:\n",
      "this film is extraordinarily horrendous and i 'm not going to waste any more words on it .\n",
      "\n",
      "---------------------------- \n",
      "| Sentiment class: 0 - NEG | \n",
      "----------------------------\n",
      "\n",
      "Human rationales / Supporting Evidence:\n",
      "     -  extraordinarily horrendous\n"
     ]
    }
   ],
   "source": [
    "def print_example(data, index, print_content=True, print_classification=True, print_rationales=True ):\n",
    "    print(f'Retrieving Training Example [{index}].................\\n')\n",
    "    item = data[index]\n",
    "    classification = item['classification']\n",
    "    evidences = item['evidences']\n",
    "    content = item['content']\n",
    "    if print_content: print(f'Review content:\\n{content}\\n')\n",
    "    if print_classification: print('----------------------------',\n",
    "                                   '\\n| Sentiment class:',\n",
    "                                   classification,\n",
    "                                   (\"- NEG\" if not classification else \"- POS\"),\n",
    "                                   '|', '\\n----------------------------')\n",
    "    if print_rationales:\n",
    "        print('\\nHuman rationales / Supporting Evidence:')\n",
    "        for evidence in evidences:\n",
    "            print('     - ', evidence[0]['text'])\n",
    "\n",
    "def get_content(data, index):\n",
    "    item = data[index]\n",
    "    content = item['content']\n",
    "    return content\n",
    "\n",
    "def get_classes(data, index):\n",
    "    item = data[index]\n",
    "    classification = item['classification']\n",
    "    return classification\n",
    "\n",
    "def get_annotations(data, index):\n",
    "    item = data[index]\n",
    "    content = item['evidences']\n",
    "    annotations = [evidence[0]['text'] for evidence in content]\n",
    "    return annotations\n",
    "\n",
    "train_size = len(train_data)\n",
    "val_size = len(validation_data)\n",
    "test_size = len(test_data)\n",
    "\n",
    "print(f'Dataset split: {train_size} training examples')\n",
    "print(f'               {val_size} validation examples')\n",
    "print(f'               {test_size} test examples\\n')\n",
    "\n",
    "print_example(train_data, 506)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLQCckZ-wz1C"
   },
   "source": [
    "# Extraction of the rationales from the evidences metadata of each human annotation of reviews.\n",
    "\n",
    "Each annotation of the review is not the highlighted text/rationale itself but also contains metadata of the text. Use the function defined in the above cell to extract just the text and replace the evidences dictionary of the training, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-5ulEzugwz1C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['extraordinarily horrendous']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_data)):\n",
    "    train_data[i]['evidences'] = get_annotations(train_data, i)\n",
    "\n",
    "for i in range(len(validation_data)):\n",
    "    validation_data[i]['evidences'] = get_annotations(validation_data, i)\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    test_data[i]['evidences'] = get_annotations(test_data, i)\n",
    "\n",
    "print(train_data[506]['evidences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vc2rC9QTytnK"
   },
   "source": [
    "# Pre-trianed GloVe Embeddings of Training Examples\n",
    "Download the pretrained GloVe Embeddings of desired dimensions using gensim downlader.\n",
    "\n",
    "Save downloaded embeddings to a local file to avoid re-downloading when the kernel or notebook is restarted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KWyAJ5D_ytnL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    ONLY if you get an error after `import gensim`: update your smart_open liberary\\n    #!conda install --yes --prefix {sys.prefix} smart_open\\n    restart your notebook\\n    see if `import gensim` works now\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Install gensim, to use word2vec word embeddings\n",
    "    Install gensim (for pre-trained word embeddings)\n",
    "    #!conda install --yes --prefix {sys.prefix} gensim\n",
    "\"\"\"\n",
    "#import gensim\n",
    "#import gensim.downloader\n",
    "\n",
    "\"\"\"\n",
    "    ONLY if you get an error after `import gensim`: update your smart_open liberary\n",
    "    #!conda install --yes --prefix {sys.prefix} smart_open\n",
    "    restart your notebook\n",
    "    see if `import gensim` works now\n",
    "\"\"\"\n",
    "#wv = gensim.downloader.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "#import pickle\n",
    "\n",
    "#with open(\"glove_embeddings.pkl\", \"wb\") as f:\n",
    "    #pickle.dump(wv, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QqA5DXEEwz1D"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.20356 , -0.8707  , -0.19172 ,  0.73862 ,  0.18494 ,  0.14926 ,\n",
       "        0.48079 , -0.21633 ,  0.72753 , -0.36912 ,  0.13397 , -0.1143  ,\n",
       "       -0.18075 , -0.64683 , -0.18484 ,  0.83575 ,  0.48179 ,  0.76026 ,\n",
       "       -0.50381 ,  0.80743 ,  1.2195  ,  0.3459  ,  0.22185 ,  0.31335 ,\n",
       "        1.2066  , -1.8441  ,  0.14064 , -0.99715 , -1.1402  ,  0.32342 ,\n",
       "        3.2128  ,  0.42708 ,  0.19504 ,  0.80113 ,  0.38555 , -0.12568 ,\n",
       "       -0.26533 ,  0.055264, -1.1557  ,  0.16836 , -0.82228 ,  0.20394 ,\n",
       "        0.089235, -0.60125 , -0.032878,  1.3735  , -0.51661 ,  0.29611 ,\n",
       "        0.23951 , -1.3801  ], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"glove_embeddings.pkl\", \"rb\") as f:\n",
    "    wv = pickle.load(f)\n",
    "\n",
    "# lookup the word vector for a word \"india\"\n",
    "wv['india']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IFRN-vp2ytnM"
   },
   "outputs": [],
   "source": [
    "# downsampled embedding and zero vector for unknown words\n",
    "# note the following code assums the the word embedding dimensions are dividible by 5\n",
    "\n",
    "import einops # type: ignore\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import types\n",
    "\n",
    "def glove_embed(word:str, target_dim)->np.array:\n",
    "    '''Looks up word in embedding (downsampled to five dimensions), pads with beginning of embedding.\n",
    "       Returns zero vector for unknown words.\n",
    "    '''\n",
    "    # these parameters work for 50-dim glove embeddings (adjust for other embeddings)\n",
    "    sampled_dim = 5\n",
    "    sample_batches = 10\n",
    "\n",
    "    empty_vec=np.zeros(target_dim)\n",
    "    if word in wv:\n",
    "        w2v = wv[word] # lookup 50 dim vector\n",
    "        a=einops.reduce(w2v,'(d seg)-> d', \"sum\", seg=sample_batches)  # downsample\n",
    "        b=w2v[0:target_dim-sampled_dim]\n",
    "        return np.hstack([a,b])\n",
    "    else:\n",
    "        return empty_vec\n",
    "\n",
    "def glove_embed_sequences(sequence, target_dim):\n",
    "\n",
    "    if isinstance(sequence, list):\n",
    "        if len(sequence) == 0:\n",
    "            empty_seq = np.zeros(target_dim)\n",
    "            gloveTensor =  torch.tensor(empty_seq, dtype=torch.float)\n",
    "        else:\n",
    "            tokens = \",\".join(sequence)\n",
    "            words = tokens.split()\n",
    "            gloveTensor = torch.stack([torch.tensor(glove_embed(word, target_dim), dtype=torch.float) for word in words])\n",
    "    else:\n",
    "        tokens = sequence.split()\n",
    "        gloveTensor = torch.stack([torch.tensor(glove_embed(token, target_dim), dtype=torch.float) for token in tokens])\n",
    "\n",
    "    return gloveTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YFDJY6pMdHx"
   },
   "source": [
    "# Extract reviews, classifications, and rationales from the train, validation, and test datasets to convert them to Glove embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XoZiHbVxytnN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews in training data: 1600\n",
      "Max seq length of reviews: 2809\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_id</th>\n",
       "      <th>classification</th>\n",
       "      <th>evidences</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negR_000.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['mind - fuck movie', 'the sad part is', 'down...</td>\n",
       "      <td>plot : two teen couples go to a church party ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negR_001.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"it 's pretty much a sunken ship\", 'sutherlan...</td>\n",
       "      <td>the happy bastard 's quick movie review damn t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negR_002.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['the characters and acting is nothing spectac...</td>\n",
       "      <td>it is movies like these that make a jaded movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negR_003.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['dead on arrival', 'the characters stink', 's...</td>\n",
       "      <td>\" quest for camelot \" is warner bros . ' first...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negR_004.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['it is highly derivative and somewhat boring'...</td>\n",
       "      <td>synopsis : a mentally unstable man undergoing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotation_id  classification  \\\n",
       "0  negR_000.txt               0   \n",
       "1  negR_001.txt               0   \n",
       "2  negR_002.txt               0   \n",
       "3  negR_003.txt               0   \n",
       "4  negR_004.txt               0   \n",
       "\n",
       "                                           evidences  \\\n",
       "0  ['mind - fuck movie', 'the sad part is', 'down...   \n",
       "1  [\"it 's pretty much a sunken ship\", 'sutherlan...   \n",
       "2  ['the characters and acting is nothing spectac...   \n",
       "3  ['dead on arrival', 'the characters stink', 's...   \n",
       "4  ['it is highly derivative and somewhat boring'...   \n",
       "\n",
       "                                             content  \n",
       "0  plot : two teen couples go to a church party ,...  \n",
       "1  the happy bastard 's quick movie review damn t...  \n",
       "2  it is movies like these that make a jaded movi...  \n",
       "3  \" quest for camelot \" is warner bros . ' first...  \n",
       "4  synopsis : a mentally unstable man undergoing ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the training dataset to a pandas dataframe\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_df.drop(columns=['query', 'query_type'], inplace=True)\n",
    "train_df['evidences'] = train_df['evidences'].astype(str)\n",
    "\n",
    "train_rationales = train_df['evidences']\n",
    "train_reviews = [get_content(train_data, i) for i in range(train_size)]\n",
    "train_classes = [get_classes(train_data, i) for i in range(train_size)]\n",
    "\n",
    "print(\"Number of reviews in training data:\",len(train_reviews))\n",
    "print(\"Max seq length of reviews:\", np.max([len(review.split()) for review in train_reviews]))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YUXL-ZR0Pujb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews in validation data: 200\n",
      "Max seq length of reviews: 1880\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_id</th>\n",
       "      <th>classification</th>\n",
       "      <th>evidences</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negR_800.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['definitely the cinematic equivalent of a sle...</td>\n",
       "      <td>there were four movies that earned jamie lee c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negR_801.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['overacts his psycho routine', 'deteriorates ...</td>\n",
       "      <td>according to hitchcock and various other filmm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negR_802.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['so dull and pedestrian and nonsensical', 'bo...</td>\n",
       "      <td>if you 've been following william fichtner 's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negR_803.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['takes the easy route out', 'most hampered no...</td>\n",
       "      <td>note : some may consider portions of the follo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negR_804.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['poor choices', \"it 's downright depressing\",...</td>\n",
       "      <td>for his directoral debut , gary oldman chose a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotation_id  classification  \\\n",
       "0  negR_800.txt               0   \n",
       "1  negR_801.txt               0   \n",
       "2  negR_802.txt               0   \n",
       "3  negR_803.txt               0   \n",
       "4  negR_804.txt               0   \n",
       "\n",
       "                                           evidences  \\\n",
       "0  ['definitely the cinematic equivalent of a sle...   \n",
       "1  ['overacts his psycho routine', 'deteriorates ...   \n",
       "2  ['so dull and pedestrian and nonsensical', 'bo...   \n",
       "3  ['takes the easy route out', 'most hampered no...   \n",
       "4  ['poor choices', \"it 's downright depressing\",...   \n",
       "\n",
       "                                             content  \n",
       "0  there were four movies that earned jamie lee c...  \n",
       "1  according to hitchcock and various other filmm...  \n",
       "2  if you 've been following william fichtner 's ...  \n",
       "3  note : some may consider portions of the follo...  \n",
       "4  for his directoral debut , gary oldman chose a...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the validation dataset to a pandas dataframe\n",
    "val_df = pd.DataFrame(validation_data)\n",
    "val_df.drop(columns=['query', 'query_type'], inplace=True)\n",
    "val_df['evidences'] = val_df['evidences'].astype(str)\n",
    "\n",
    "val_rationales = val_df['evidences']\n",
    "val_reviews = [get_content(validation_data, i) for i in range(val_size)]\n",
    "val_classes = [get_classes(validation_data, i) for i in range(val_size)]\n",
    "\n",
    "print(\"Number of reviews in validation data:\",len(val_reviews))\n",
    "print(\"Max seq length of reviews:\", np.max([len(review.split()) for review in val_reviews]))\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Q4vG15P9OvtM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews in test data: 199\n",
      "Max seq length of reviews: 2122\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_id</th>\n",
       "      <th>classification</th>\n",
       "      <th>evidences</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negR_900.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['i even giggled']</td>\n",
       "      <td>there may not be a critic alive who harbors as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negR_901.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['rings']</td>\n",
       "      <td>renee zellweger stars as sonia , a young jewis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negR_902.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"there 're so many things to criticize about ...</td>\n",
       "      <td>there 're so many things to criticize about i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negR_903.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"do n't let this movie fool you into believin...</td>\n",
       "      <td>do n't let this movie fool you into believing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negR_904.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"is proof that hollywood does n't have a clue...</td>\n",
       "      <td>it 's a good thing most animated sci - fi movi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotation_id  classification  \\\n",
       "0  negR_900.txt               0   \n",
       "1  negR_901.txt               0   \n",
       "2  negR_902.txt               0   \n",
       "3  negR_903.txt               0   \n",
       "4  negR_904.txt               0   \n",
       "\n",
       "                                           evidences  \\\n",
       "0                                 ['i even giggled']   \n",
       "1                                          ['rings']   \n",
       "2  [\"there 're so many things to criticize about ...   \n",
       "3  [\"do n't let this movie fool you into believin...   \n",
       "4  [\"is proof that hollywood does n't have a clue...   \n",
       "\n",
       "                                             content  \n",
       "0  there may not be a critic alive who harbors as...  \n",
       "1  renee zellweger stars as sonia , a young jewis...  \n",
       "2  there 're so many things to criticize about i ...  \n",
       "3  do n't let this movie fool you into believing ...  \n",
       "4  it 's a good thing most animated sci - fi movi...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the test dataset to a pandas dataframe\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_df.drop(columns=['docids','query', 'query_type'], inplace=True)\n",
    "test_df['evidences'] = test_df['evidences'].astype(str)\n",
    "\n",
    "test_rationales = test_df['evidences']\n",
    "test_reviews = [get_content(test_data, i) for i in range(test_size)]\n",
    "test_classes = [get_classes(test_data, i) for i in range(test_size)]\n",
    "\n",
    "print(\"Number of reviews in test data:\",len(test_reviews))\n",
    "print(\"Max seq length of reviews:\", np.max([len(review.split()) for review in test_reviews]))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "iqk66GB4wz1E"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Find the smallest review of all to inspect and understand the training data structure.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m smallest_entry \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlen\u001b[39m)\u001b[38;5;241m.\u001b[39midxmin()]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(smallest_entry)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegR_506.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Find the smallest review of all to inspect and understand the training data structure.\n",
    "smallest_entry = df.loc[df['content'].apply(len).idxmin()]\n",
    "print(smallest_entry)\n",
    "id = 'negR_506.txt'\n",
    "index = df['content'].apply(len).idxmin()\n",
    "print(f\"{df.iloc[506]['content']} \\n{df.iloc[506]['classification']} \\n{df.iloc[506]['evidences']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_WbCl45wz1E"
   },
   "source": [
    "Extract validation set from the val.jsonl file and create a dataframe for it similar to the training set and save it to a csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbHzxzSlwz1E"
   },
   "source": [
    "# Convert the reviews to their corresponding Glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pm21m4EZwz1F"
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_review_gloves = [glove_embed_sequences(train_reviews, 11) for review in tqdm(train_df['content'])]\n",
    "train_review_gloves = pad_sequence(train_review_gloves, batch_first=True)\n",
    "\n",
    "with open(\"train_reviews.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_review_gloves, f)\n",
    "\n",
    "val_review_gloves = [glove_embed_sequences(val_reviews, 11) for review in tqdm(val_df['content'])]\n",
    "val_review_gloves = pad_sequence(val_review_gloves, batch_first=True)\n",
    "\n",
    "with open(\"val_reviews.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_review_gloves, f)\n",
    "\n",
    "test_review_gloves = [glove_embed_sequences(test_reviews, 11) for review in tqdm(test_df['content'])]\n",
    "test_review_gloves = pad_sequence(test_review_gloves, batch_first=True)\n",
    "\n",
    "with open(\"test_reviews.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_review_gloves, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBFXaHfPwz1F"
   },
   "source": [
    "# Convert the rationales to their corresponding Glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "et3YJVo8wz1F"
   },
   "outputs": [],
   "source": [
    "train_rationale_gloves = [glove_embed_sequences(rationale, 11) for rationale in tqdm(train_df['evidences'])]\n",
    "train_rationale_gloves = pad_sequence(train_rationale_gloves, batch_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSzDGF43ytnN"
   },
   "source": [
    "# Convert the training data to batches using DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wsgVwakDytnO"
   },
   "outputs": [],
   "source": [
    "input = reviewGloVes\n",
    "print(\"input dim (batches, max_seq_len, embed_size):\",input.size())\n",
    "Y_star = torch.tensor(classes, dtype=torch.float)\n",
    "train_x = TensorDataset(input, Y_star)\n",
    "dataLoader = DataLoader(train_x, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPQy0BjBytnO"
   },
   "source": [
    "# Convolutional Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smXuMplUytnO"
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "from torch import Tensor\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Tuple[int, ...],\n",
    "        pool_size: Tuple[int, ...],\n",
    "        stride: Tuple[int, ...],\n",
    "        padding: Tuple[int, ...],\n",
    "        bias: bool\n",
    "    ) -> None:\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "\n",
    "        super(MyModel, self).__init__()\n",
    "        self.ReLU_Activation = nn.ReLU()\n",
    "        self.convolutionLayer = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.poolingLayer = nn.MaxPool1d(kernel_size=pool_size, stride=stride, padding=padding)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        X: Tensor,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Embedding Layer X --Theta--> E\n",
    "        Input: X (Tensor)\n",
    "        Output: E (Tensor)\n",
    "        Parameter: Theta (optional)\n",
    "        \"\"\"\n",
    "\n",
    "        X = X.squeeze(0)\n",
    "        X_shape = list(X.size())\n",
    "        #print(X_shape)\n",
    "        #print(X)\n",
    "        self.embeddingLayer = nn.Linear(2809, self.in_channels, bias=False)\n",
    "        self.embeddingLayer.weight.data = torch.randn((3,11),dtype=torch.float)\n",
    "        E = self.embeddingLayer(X)\n",
    "        #print(E)\n",
    "\n",
    "        E_shape = list(E.size())\n",
    "        if len(E_shape) < 3:\n",
    "            E = E.unsqueeze(0)\n",
    "        E = E.permute(0, 2, 1)\n",
    "\n",
    "        \"\"\"\n",
    "        Convolution Layer E --W--> H\n",
    "        Input: E (Tensor)\n",
    "        Output: H (Tensor)\n",
    "        Parameter: W (optional)\n",
    "        \"\"\"\n",
    "        #self.convolutionLayer.weight.data = W_torch.permute(2, 0, 1)\n",
    "        H = self.convolutionLayer(E)\n",
    "        #print(H)\n",
    "        H = H.squeeze(0)\n",
    "        H = H.permute(1, 0)\n",
    "\n",
    "        \"\"\"\n",
    "        Dectector Layer H --Psi--> D with ReLU\n",
    "        Input: H (Tensor)\n",
    "        Output: D (Tensor)\n",
    "        Parameter: Psi (optional)\n",
    "        \"\"\"\n",
    "        myPsi_torch = torch.randn((2,1), dtype=torch.float)\n",
    "        D = torch.einsum('ij,jk->ik', H, myPsi_torch)\n",
    "        self.ReLU_Activation = nn.ReLU()\n",
    "        D = self.ReLU_Activation(D)\n",
    "        #print(D)\n",
    "        D = D.permute(1, 0)\n",
    "\n",
    "        \"\"\"\n",
    "        Pooling Layer D --MaxPool--> Y\n",
    "        Input: D (Tensor)\n",
    "        output: Y (Tensor)\n",
    "        Parameter: Pooling_window (optional)\n",
    "        \"\"\"\n",
    "        P = self.poolingLayer(D)\n",
    "        Y_hat = torch.zeros(X_shape[0])\n",
    "        D_shape = list(D.size())\n",
    "        out_size =  D_shape[1] - self.pool_size // self.stride + 1\n",
    "        Y_hat[:out_size] = P\n",
    "\n",
    "        return Y_hat\n",
    "\n",
    "model = MyModel(3, 2, 3, 2, 1, 0, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOONmTg-ytnP"
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0IXthmm0ytnP"
   },
   "outputs": [],
   "source": [
    "def train(xdata, ydata):\n",
    "    '''Train the neural model with the given training data'''\n",
    "\n",
    "    #Construct the loss function\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    # Construct the optimizer (Stochastic Gradient Descent in this case)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)  # lr is learning rate\n",
    "\n",
    "    # Gradient Descent\n",
    "    for epoch in range(201):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        Y_pred = model(xdata)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(Y_pred, ydata)\n",
    "\n",
    "        if epoch >0 and epoch % 40 == 0:\n",
    "            print('epoch: ', epoch,' loss: ', loss.item())\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # perform a backward pass (backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    return Y_pred\n",
    "\n",
    "batch_input = torch.zeros((2809, 11), dtype=torch.float)\n",
    "predList = []\n",
    "batch_outputs = torch.zeros(5, dtype=torch.float)\n",
    "k = 0\n",
    "for batch in dataLoader:\n",
    "    batch_input = batch[0]\n",
    "    y_star = Y_star[k]\n",
    "    k += 1\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    print(\"Training batch:\", k)\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    Y_pred = train(batch_input, y_star)\n",
    "    #print(Y_pred.size())\n",
    "    predList.append(Y_pred[:5])\n",
    "    if k == 5:\n",
    "        break\n",
    "batch_outputs = torch.stack(predList)\n",
    "#print(batch_outputs.size())\n",
    "print(\"======================================================================\")\n",
    "mse=torch.mean( (batch_outputs-Y_star[:5])**2)  # detach takes the tensor out of the network\n",
    "print(\"MSE for ground truth y_star\", mse)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
