{
<<<<<<< HEAD
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Nikhil-Kadapala/NeuralNets/blob/main/standardNeuralNets/stdCNN_LIME.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
=======
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhil-Kadapala/NeuralNets/blob/main/standardNeuralNets/stdCNN_LIME.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L2Do3b_ytm6"
      },
      "source": [
        "# Final Project CS852 - Foundations of Neural Networks (FALL 2024)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICkcYT3Iytm9"
      },
      "source": [
        "Devin Borchard and Nikhil Kadapala\n",
        "\n",
        "Department of Computer Science, University of New Hampshire\n",
        "\n",
        "ERASER datasets: https://www.eraserbenchmark.com/\n",
        "\n",
        "ERASER paper: https://arxiv.org/pdf/1911.03429\n",
        "\n",
        "LIME paper: https://arxiv.org/pdf/1602.04938"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr8uEiG3ytm9"
      },
      "source": [
        "# Notebook setup and PyTorch Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LJ5b9esmytm-"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# uncomment one of these versions (depending on whether you are on a computer with a CPU or not)\n",
        "\n",
        "# GPU version\n",
        "# !conda install --yes --prefix {sys.prefix} pytorch torchvision cudatoolkit=10.2 -c pytorch\n",
        "\n",
        "# Just CPU\n",
        "# !conda install --yes --prefix {sys.prefix} pytorch torchvision cpuonly -c pytorch\n",
        "\n",
        "# install `Einops` for einstein-style tensor manipulation in pytorch\n",
        "# Also see https://github.com/arogozhnikov/einops\n",
        "# !conda install --yes --prefix {sys.prefix} einops  -c conda-forge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSKM3pNDytnA",
        "outputId": "6833a400-81fd-4a51-8114-932d13780a3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1451, 0.4393, 0.2234],\n",
            "        [0.3007, 0.9026, 0.2354],\n",
            "        [0.7585, 0.2750, 0.3510],\n",
            "        [0.9434, 0.5471, 0.0845],\n",
            "        [0.9596, 0.5305, 0.8646]])\n",
            "GPU/CUDA available?  True\n",
            "Torch version 2.5.1+cu121\n"
          ]
        }
      ],
      "source": [
        "# torch test\n",
        "import torch\n",
        "x = torch.rand(5, 3)\n",
        "print(x)\n",
        "\n",
        "print(\"GPU/CUDA available? \", torch.cuda.is_available())\n",
        "\n",
        "print(\"Torch version\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIw0tI1IytnB"
      },
      "source": [
        "# **Extracting Traning, Validation, and Test Data**\n",
        "# Parse the data files to extract the reviews, classifications and annotations for each split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIrDTApeytnC"
      },
      "source": [
        "There are three files:\n",
        "- train.jsonl: containts 1600 training examples\n",
        "- val.jsonl: contains 200 validation examples\n",
        "- test.json: contains 199 test examples\n",
        "\n",
        "Each example includes:\n",
        "- annotation_id: a unique id for an example of the form negR_000 for negative examples and posR_000 for positive examples.\n",
        "- evidences: a list of rationales(specific parts of the review) given by humans that most influenced their classification decision.\n",
        "- classification: the class of the example\n",
        "\n",
        "The annotation_id of each example is the name of the file for the input text data\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pVM3qH6FytnD"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def parse_data(file_path):\n",
        "    data = []                                               # Initialize an empty list to store the dictionaries\n",
        "\n",
        "    with open(file_path, 'r') as file:                      # Open the .jsonl file and read it line by line\n",
        "        for line in file:\n",
        "            annotation = json.loads(line)                   # Parse each line as JSON and append it to the list\n",
        "            id = annotation[\"annotation_id\"]\n",
        "            annotation[\"classification\"] = 1 if annotation['classification'] == \"POS\" else 0\n",
        "\n",
        "            with open(f\"./movies/docs/{id}\", 'r') as file:  # open the file named by annotation_id to extract the review text\n",
        "                content = file.read()\n",
        "                annotation['content'] = content.replace('\\n', ' ')\n",
        "                data.append(annotation)\n",
        "    return data\n",
        "\n",
        "# Specify the path to your JSON file\n",
        "train_file_path = './movies/train.jsonl'\n",
        "val_file_path = './movies/val.jsonl'\n",
        "test_file_path = './movies/test.jsonl'\n",
        "\n",
        "train_data = parse_data(train_file_path)\n",
        "validation_data = parse_data(val_file_path)\n",
        "test_data = parse_data(test_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bijy4N-HytnD"
      },
      "source": [
        "# Functions to extract reviews, classifications, and annotations\n",
        "  Define a function\n",
        "\n",
        "  i) to retrieve an example and print the relevant information.\n",
        "\n",
        "  ii) to retrieve the content of the example review text\n",
        "\n",
        "  iii) to retrieve the classifications of the examples\n",
        "  \n",
        "  iv) to retrieve the annotations provided to support the classifications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hKGZxYzTytnE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1858ca-52b3-4c4b-a5b3-11a653a6faa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset split: 1600 training examples\n",
            "               200 validation examples\n",
            "               199 test examples\n",
            "\n",
            "Retrieving Training Example [506].................\n",
            "\n",
            "Review content:\n",
            "this film is extraordinarily horrendous and i 'm not going to waste any more words on it .\n",
            "\n",
            "---------------------------- \n",
            "| Sentiment class: 0 - NEG | \n",
            "----------------------------\n",
            "\n",
            "Human rationales / Supporting Evidence:\n",
            "     -  extraordinarily horrendous\n"
          ]
        }
      ],
      "source": [
        "def print_example(data, index, print_content=True, print_classification=True, print_rationales=True ):\n",
        "    print(f'Retrieving Training Example [{index}].................\\n')\n",
        "    item = data[index]\n",
        "    classification = item['classification']\n",
        "    evidences = item['evidences']\n",
        "    content = item['content']\n",
        "    if print_content: print(f'Review content:\\n{content}\\n')\n",
        "    if print_classification: print('----------------------------',\n",
        "                                   '\\n| Sentiment class:',\n",
        "                                   classification,\n",
        "                                   (\"- NEG\" if not classification else \"- POS\"),\n",
        "                                   '|', '\\n----------------------------')\n",
        "    if print_rationales:\n",
        "        print('\\nHuman rationales / Supporting Evidence:')\n",
        "        for evidence in evidences:\n",
        "            print('     - ', evidence[0]['text'])\n",
        "\n",
        "def get_content(data, index):\n",
        "    item = data[index]\n",
        "    content = item['content']\n",
        "    return content\n",
        "\n",
        "def get_classes(data, index):\n",
        "    item = data[index]\n",
        "    classification = item['classification']\n",
        "    return classification\n",
        "\n",
        "def get_annotations(data, index):\n",
        "    item = data[index]\n",
        "    content = item['evidences']\n",
        "    annotations = [evidence[0]['text'] for evidence in content]\n",
        "    return annotations\n",
        "\n",
        "train_size = len(train_data)\n",
        "val_size = len(validation_data)\n",
        "test_size = len(test_data)\n",
        "\n",
        "print(f'Dataset split: {train_size} training examples')\n",
        "print(f'               {val_size} validation examples')\n",
        "print(f'               {test_size} test examples\\n')\n",
        "\n",
        "print_example(train_data, 506)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLQCckZ-wz1C"
      },
      "source": [
        "# Extraction of the rationales from the evidences metadata of each human annotation of reviews.\n",
        "\n",
        "Each annotation of the review is not the highlighted text/rationale itself but also contains metadata of the text. Use the function defined in the above cell to extract just the text and replace the evidences dictionary of the training, validation, and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-5ulEzugwz1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0afafd66-881a-46a8-ef91-1aae4fd87cc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['extraordinarily horrendous']\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(train_data)):\n",
        "    train_data[i]['evidences'] = get_annotations(train_data, i)\n",
        "\n",
        "for i in range(len(validation_data)):\n",
        "    validation_data[i]['evidences'] = get_annotations(validation_data, i)\n",
        "\n",
        "for i in range(len(test_data)):\n",
        "    test_data[i]['evidences'] = get_annotations(test_data, i)\n",
        "\n",
        "print(train_data[506]['evidences'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc2rC9QTytnK"
      },
      "source": [
        "# Pre-trianed GloVe Embeddings of Training Examples\n",
        "Download the pretrained GloVe Embeddings of desired dimensions using gensim downlader.\n",
        "\n",
        "Save downloaded embeddings to a local file to avoid re-downloading when the kernel or notebook is restarted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KWyAJ5D_ytnL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "12975316-bbe9-4a4b-f10f-2f3cdabb298a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    ONLY if you get an error after `import gensim`: update your smart_open liberary\\n    #!conda install --yes --prefix {sys.prefix} smart_open\\n    restart your notebook\\n    see if `import gensim` works now\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\"\"\"\n",
        "    Install gensim, to use word2vec word embeddings\n",
        "    Install gensim (for pre-trained word embeddings)\n",
        "    #!conda install --yes --prefix {sys.prefix} gensim\n",
        "\"\"\"\n",
        "#import gensim\n",
        "#import gensim.downloader\n",
        "\n",
        "\"\"\"\n",
        "    ONLY if you get an error after `import gensim`: update your smart_open liberary\n",
        "    #!conda install --yes --prefix {sys.prefix} smart_open\n",
        "    restart your notebook\n",
        "    see if `import gensim` works now\n",
        "\"\"\"\n",
        "#wv = gensim.downloader.load(\"glove-wiki-gigaword-50\")\n",
        "\n",
        "#import pickle\n",
        "\n",
        "#with open(\"glove_embeddings.pkl\", \"wb\") as f:\n",
        "    #pickle.dump(wv, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QqA5DXEEwz1D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb1ba4b6-8085-456c-89bd-70b3e10dd471"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.20356 , -0.8707  , -0.19172 ,  0.73862 ,  0.18494 ,  0.14926 ,\n",
              "        0.48079 , -0.21633 ,  0.72753 , -0.36912 ,  0.13397 , -0.1143  ,\n",
              "       -0.18075 , -0.64683 , -0.18484 ,  0.83575 ,  0.48179 ,  0.76026 ,\n",
              "       -0.50381 ,  0.80743 ,  1.2195  ,  0.3459  ,  0.22185 ,  0.31335 ,\n",
              "        1.2066  , -1.8441  ,  0.14064 , -0.99715 , -1.1402  ,  0.32342 ,\n",
              "        3.2128  ,  0.42708 ,  0.19504 ,  0.80113 ,  0.38555 , -0.12568 ,\n",
              "       -0.26533 ,  0.055264, -1.1557  ,  0.16836 , -0.82228 ,  0.20394 ,\n",
              "        0.089235, -0.60125 , -0.032878,  1.3735  , -0.51661 ,  0.29611 ,\n",
              "        0.23951 , -1.3801  ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"glove_embeddings.pkl\", \"rb\") as f:\n",
        "    wv = pickle.load(f)\n",
        "\n",
        "# lookup the word vector for a word \"india\"\n",
        "wv['india']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IFRN-vp2ytnM"
      },
      "outputs": [],
      "source": [
        "# downsampled embedding and zero vector for unknown words\n",
        "# note the following code assums the the word embedding dimensions are dividible by 5\n",
        "\n",
        "import einops # type: ignore\n",
        "import numpy as np\n",
        "from typing import List\n",
        "import types\n",
        "\n",
        "def glove_embed(word:str, target_dim)->np.array:\n",
        "    '''Looks up word in embedding (downsampled to five dimensions), pads with beginning of embedding.\n",
        "       Returns zero vector for unknown words.\n",
        "    '''\n",
        "    # these parameters work for 50-dim glove embeddings (adjust for other embeddings)\n",
        "    sampled_dim = 5\n",
        "    sample_batches = 10\n",
        "\n",
        "    empty_vec=np.zeros(target_dim)\n",
        "    if word in wv:\n",
        "        w2v = wv[word] # lookup 50 dim vector\n",
        "        a=einops.reduce(w2v,'(d seg)-> d', \"sum\", seg=sample_batches)  # downsample\n",
        "        b=w2v[0:target_dim-sampled_dim]\n",
        "        return np.hstack([a,b])\n",
        "    else:\n",
        "        return empty_vec\n",
        "\n",
        "def glove_embed_sequences(sequence, target_dim):\n",
        "\n",
        "    if isinstance(sequence, list):\n",
        "        if len(sequence) == 0:\n",
        "            empty_seq = np.zeros(target_dim)\n",
        "            gloveTensor =  torch.tensor(empty_seq, dtype=torch.float)\n",
        "        else:\n",
        "            tokens = \",\".join(sequence)\n",
        "            words = tokens.split()\n",
        "            gloveTensor = torch.stack([torch.tensor(glove_embed(word, target_dim), dtype=torch.float) for word in words])\n",
        "    else:\n",
        "        tokens = sequence.split()\n",
        "        gloveTensor = torch.stack([torch.tensor(glove_embed(token, target_dim), dtype=torch.float) for token in tokens])\n",
        "\n",
        "    return gloveTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YFDJY6pMdHx"
      },
      "source": [
        "# Extract reviews, classifications, and rationales from the train, validation, and test datasets to convert them to Glove embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XoZiHbVxytnN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "329dacdb-7801-4204-f2e4-1b26231dba21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of reviews in training data: 1600\n",
            "Max seq length of reviews: 2809\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  annotation_id  classification  \\\n",
              "0  negR_000.txt               0   \n",
              "1  negR_001.txt               0   \n",
              "2  negR_002.txt               0   \n",
              "3  negR_003.txt               0   \n",
              "4  negR_004.txt               0   \n",
              "\n",
              "                                           evidences  \\\n",
              "0  ['mind - fuck movie', 'the sad part is', 'down...   \n",
              "1  [\"it 's pretty much a sunken ship\", 'sutherlan...   \n",
              "2  ['the characters and acting is nothing spectac...   \n",
              "3  ['dead on arrival', 'the characters stink', 's...   \n",
              "4  ['it is highly derivative and somewhat boring'...   \n",
              "\n",
              "                                             content  \n",
              "0  plot : two teen couples go to a church party ,...  \n",
              "1  the happy bastard 's quick movie review damn t...  \n",
              "2  it is movies like these that make a jaded movi...  \n",
              "3  \" quest for camelot \" is warner bros . ' first...  \n",
              "4  synopsis : a mentally unstable man undergoing ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-080fa0eb-df7e-4d62-b14d-2047a3306dc2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>annotation_id</th>\n",
              "      <th>classification</th>\n",
              "      <th>evidences</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negR_000.txt</td>\n",
              "      <td>0</td>\n",
              "      <td>['mind - fuck movie', 'the sad part is', 'down...</td>\n",
              "      <td>plot : two teen couples go to a church party ,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>negR_001.txt</td>\n",
              "      <td>0</td>\n",
              "      <td>[\"it 's pretty much a sunken ship\", 'sutherlan...</td>\n",
              "      <td>the happy bastard 's quick movie review damn t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negR_002.txt</td>\n",
              "      <td>0</td>\n",
              "      <td>['the characters and acting is nothing spectac...</td>\n",
              "      <td>it is movies like these that make a jaded movi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negR_003.txt</td>\n",
              "      <td>0</td>\n",
              "      <td>['dead on arrival', 'the characters stink', 's...</td>\n",
              "      <td>\" quest for camelot \" is warner bros . ' first...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negR_004.txt</td>\n",
              "      <td>0</td>\n",
              "      <td>['it is highly derivative and somewhat boring'...</td>\n",
              "      <td>synopsis : a mentally unstable man undergoing ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-080fa0eb-df7e-4d62-b14d-2047a3306dc2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-080fa0eb-df7e-4d62-b14d-2047a3306dc2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-080fa0eb-df7e-4d62-b14d-2047a3306dc2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dc479233-6ada-44d8-9398-24c5e99a97df\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dc479233-6ada-44d8-9398-24c5e99a97df')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dc479233-6ada-44d8-9398-24c5e99a97df button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 1600,\n  \"fields\": [\n    {\n      \"column\": \"annotation_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1600,\n        \"samples\": [\n          \"negR_526.txt\",\n          \"negR_354.txt\",\n          \"negR_168.txt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"classification\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"evidences\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1600,\n        \"samples\": [\n          \"['so full of plot holes and non characters as to be sure to be the recipient of next years \\\" razzie \\\" award', 'there is so much wrong with this film', 'reeks almost as bad as the piles of rotting fish used to trap the beast', 'the movie has little or no depth', 'plot holes', 'should be ashamed of themselves', 'bleak and ugly looking', \\\"i sincerely hope that i 've completely spoiled any interest\\\", 'without a doubt the loudest , longest , and ultimately most amateurishly written film ever released', 'the performances in the film are singularly bland', 'without a doubt the most brain dead motion picture of the decade']\",\n          \"[\\\"wow , a film without any redeeming qualities whatsoever . i 'm amazed that someone thought this was a story that must be told on screen\\\", \\\"even i 'm offended by it\\\", \\\"there 's just an endless stream of profanity and naked breasts\\\", 'there was no comedy in the film']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1599,\n        \"samples\": [\n          \"moviemaking is a lot like being the general manager of an nfl team in the post - salary cap era -- you 've got to know how to allocate your resources . every dollar spent on a free - agent defensive tackle is one less dollar than you can spend on linebackers or safeties or centers . in the nfl , this leads to teams like the detroit lions , who boast a superstar running back with a huge contract , but can only field five guys named herb to block for him . in the movies , you end up with films like \\\" spawn \\\" , with a huge special - effects budget but not enough money to hire any recognizable actors . jackie chan is the barry sanders of moviemaking . he spins and darts across the screen like sanders cutting back through the defensive line . watching jackie in operation condor as he drives his motorcycle through the crowded streets of madrid , fleeing an armada of pursuers in identical black compact cars , is reminiscent of sanders running for daylight with the chicago bears in hot pursuit , except that sanders does n't have to worry about rescuing runaway baby carriages . but like the lions star , jackie does n't have anybody to block for him . almost every cent that 's invested in a jackie chan movie goes for stunts , and as chan does his own stunts , the rest of the money goes to pay his hospital bills . this leaves about 75 cents to pay for things like directors ( chan directs ) , scripts and dubbing and supporting characters , not to mention the hideous title sequence . this also explains why the movie was shot in odd places like morocco and spain . ( chan 's first release in this country , \\\" rumble in the bronx \\\" , was supposedly set in new york , but was filmed in vancouver , and in the chase scenes the canadian rockies are clearly visible . ) heck , jackie does n't even have enough money for a haircut , looks like , much less a personal hairstylist . in condor , chan plays the same character he 's always played , himself , a mixture of bruce lee and tim allen , a master of both kung - fu and slapstick - fu . jackie is sent by the un to retrieve a cache of lost nazi gold in the north african desert , and is chased by a horde of neo - nazi sympathizers and two stereotypical arabs ( one of the things i like about jackie chan movies : no political correctness ) . he is joined by three women , who have little to do except scream , \\\" jackie , save us ! \\\" , and misuse firearms . the villain is an old nazi whose legs were broken in the secret base so that he has to be carried everywhere , and he 's more pathetic than evil . en route , we have an extended motorcycle chase scene , a hilarious fight in the moroccan version of motel 6 with the neo - nazis , and two confrontations with savage natives . once at the secret desert base , there is a long chop - socky sequence , followed by the film 's centerpiece , a wind - tunnel fight that 's even better than the one in face / off . this is where the money was spent , on well - choreographed kung - fu sequences , on giant kevlar hamster balls , on smashed - up crates of bananas , and on scorpions . ignore the gaping holes in the plot ( how , exactly , if the villain 's legs were broken , did he escape from the secret nazi base , and why did n't he take the key with him ? ) . do n't worry about the production values , or what , exactly , the japanese girl was doing hitchhiking across the sahara . just go see the movie . operation condor has pretentions of being a \\\" raiders of the lost ark \\\" knockoff , but one wonders what jackie could do with the raiders franchise blocking for him -- with a lawrence kazdan screenplay , a john williams score , spielberg directing and george lucas producing , condor might be an a+ movie . however , you 've got to go with what you 've got , and what you 've got in jackie chan is something special -- a talent that mainstream hollywood should , could , and ought to utilize .\",\n          \"\\\" something is fishy in the state of universal . \\\" about ten years back , with the unexpected success of mad max and the road warrior , post - apocalypse nitty - gritty survival yarns became popular at the movies . we 've always had movies of this nature ; on the beach , the end of the world , damnation alley , the ultimate warrior , and so on . to date , the most smoothly done were straightforward \\\" haircuts \\\" of the classic western plot , like the lone gunman who comes to town and protects the widow and the son against an evil organization , usually one in possession of some critical resource , like water , feed range , or a mining claim . most of these grew out of venerable , but solid hero yarns like the virginian and shane . ( my personal favorite is a patrick swayze movie called steel dawn , which was fairly well made on a small budget . ) now we have waterworld , which again brings the traditional lone gunman to town to rescue the young widow and her daughter . ( well , she 's not a widow , and the kid is n't her daughter , but you get the idea . ) the lady is helen , played by the stunning jean tripplehorn , who is n't given a chance to be stunning , or even interesting , by the mediocre and unimaginative script . the child enola , played by tina majorino , is living proof that a child actor need not be a bad thing to have in a movie ; she outshines her material all the way through . in simple , the scene is earth , hundreds of years from now . the polar ice caps have melted , and somehow produced enough water to inundate the entire planet . the few remaining people live in boats and floating colonies , and survive by trade , theft , or piracy . somehow an oil tanker has survived the centuries , and its inhabitants , called \\\" smokers , \\\" are able to keep gasoline engines running despite the dearth of replacement parts and raw materials , so the bad guys have outboard engines , and fast - moving boats , airplanes , and jet skis . enola , found at sea as a young girl , has a mysterious map no one can read tattooed on her back . we suspect early on that it is the way to the mythical \\\" dryland , \\\" the place where trees , crops , and animals grow , and what plot there is hinges on who has enola . the psycho ruler of the smokers , the \\\" deacon , \\\" is trying to get her and find his way to dryland . played with typical self - lampooning , rug - chewing histrionics by dennis hopper , \\\" deacon \\\" is the only thing in the movie that 's close to amusing . his performance is * almost * laughable , but there just is n't enough there to be funny . the star ( and a co - producer ) is kevin costner . he 's playing an un - named lone denizen of the sea , a man called the \\\" mariner , \\\" who turns out to be a gilled , water - breathing mutant with webbed feet . very little is done with this . the script ignores the ineffectuality of gills in supplying enough oxygen to support a human metabolism ; it ignores the fact that even with both ice caps completely melted , much of the earth 's surface would still be above water ; and it ignores the blatant impossibility of the cultures and technology shown . ( canned meat does * not * last for centuries ; ammunition does * not * fire after it 's more than a few decades old ; and so on , and so on . . . ) i 'm quite fond of tina majorino 's previous work , very impressed by jean tripplehorn 's past accomplishments , and still speechless over costner 's dances with wolves . but this movie could destroy the careers of anyone associated with it ! this movie cost one hundred and eighty - two million dollars , and there 's * nothing * in it we have n't seen before , done better on only a few percent of the cost of this turkey . at 125 minutes of material , this movie cost over one point four million dollars per minute to make . the budget of this movie * could * have given us over thirty movies ; it could have paid for six years of a prime - time sf tv series with expensive fx work , or ten years of an sf tv series with good digital fx . in sum , this movie is beneath contempt . it has nothing new to offer , it has a script that could easily have been bettered by the people who write comic books for dc , and it spent more money than the national budget of a small nation . if you * have * to go see it , see it on a four - dollar matinee . otherwise you 'll find yourself sneering at you every time you pass a reflective surface , for weeks .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# convert the training dataset to a pandas dataframe\n",
        "train_df = pd.DataFrame(train_data)\n",
        "train_df.drop(columns=['query', 'query_type'], inplace=True)\n",
        "train_df['evidences'] = train_df['evidences'].astype(str)\n",
        "\n",
        "train_rationales = train_df['evidences']\n",
        "train_reviews = [get_content(train_data, i) for i in range(train_size)]\n",
        "train_classes = [get_classes(train_data, i) for i in range(train_size)]\n",
        "\n",
        "print(\"Number of reviews in training data:\",len(train_reviews))\n",
        "print(\"Max seq length of reviews:\", np.max([len(review.split()) for review in train_reviews]))\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YUXL-ZR0Pujb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "outputId": "ad4272f7-752a-4542-eecd-ca47427a8072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of reviews in validation data: 200\n",
            "Max seq length of reviews: 1880\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  annotation_id  classification  \\\n",
              "0  negR_800.txt               0   \n",
              "1  negR_801.txt               0   \n",
              "2  negR_802.txt               0   \n",
              "3  negR_803.txt               0   \n",
              "4  negR_804.txt               0   \n",
              "\n",
              "                                           evidences  \\\n",
              "0  ['definitely the cinematic equivalent of a sle...   \n",
              "1  ['overacts his psycho routine', 'deteriorates ...   \n",
              "2  ['so dull and pedestrian and nonsensical', 'bo...   \n",
              "3  ['takes the easy route out', 'most hampered no...   \n",
              "4  ['poor choices', \"it 's downright depressing\",...   \n",
              "\n",
              "                                             content  \n",
              "0  there were four movies that earned jamie lee c...  \n",
              "1  according to hitchcock and various other filmm...  \n",
              "2  if you 've been following william fichtner 's ...  \n",
              "3  note : some may consider portions of the follo...  \n",
              "4  for his directoral debut , gary oldman chose a...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a4e606cf-d44c-4284-948b-5868dac9f932\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>annotation_id</th>\n",
              "      <th>classification</th>\n",
              "      <th>evidences</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negR_800.txt</td>\n",
              "      <td>0</td>\n",
              "      <td>['definitely the cinematic equivalent of a sle...</td>\n",
              "      <td>there were four movies that earned jamie lee c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>negR_801.txt</td>\n",
              "      <td>0</td>\n",
              "      <td>['overacts his psycho routine', 'deteriorates ...</td>\n",
              "      <td>according to hitchcock and various other filmm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negR_802.txt</td>\n",
              "      <td>0</td>\n",
              "      <td>['so dull and pedestrian and nonsensical', 'bo...</td>\n",
              "      <td>if you 've been following william fichtner 's ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negR_803.txt</td>\n",
              "      <td>0</td>\n",
              "      <td>['takes the easy route out', 'most hampered no...</td>\n",
              "      <td>note : some may consider portions of the follo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negR_804.txt</td>\n",
              "      <td>0</td>\n",
              "      <td>['poor choices', \"it 's downright depressing\",...</td>\n",
              "      <td>for his directoral debut , gary oldman chose a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4e606cf-d44c-4284-948b-5868dac9f932')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a4e606cf-d44c-4284-948b-5868dac9f932 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a4e606cf-d44c-4284-948b-5868dac9f932');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4e4440f3-f670-4a5f-997b-78a326b806b9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4e4440f3-f670-4a5f-997b-78a326b806b9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4e4440f3-f670-4a5f-997b-78a326b806b9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "val_df",
              "summary": "{\n  \"name\": \"val_df\",\n  \"rows\": 200,\n  \"fields\": [\n    {\n      \"column\": \"annotation_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"negR_895.txt\",\n          \"negR_815.txt\",\n          \"negR_830.txt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"classification\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"evidences\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"['a pulp fiction knock off', \\\"the story is n't much , and the dialogue and characters rate only marginally better\\\", \\\"a movie is n't that good\\\", 'i really cared absolutely nothing about', 'prepare for disappointment', 'falls way short', 'they inevitably are brought down']\",\n          \"['this . . . this . . . thing .', 'so flatly drawn', 'a collection of unfunny cringe - inducing moments coupled with uninsightful cringe - inducing moments', 'astoundingly implausible', 'an unfunny , uninsightful , cringe - inducing , thoroughly icky embarrassment', \\\"forget about what we 've seen here\\\", \\\"let 's all join hands and pray that the planet these folks are from is not this one .\\\", 'the most difficult to watch', 'wades in the bog of stupidity']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"2 days in the valley is more or less a pulp fiction knock off . it basically involves how a bunch of quirky characters in the los angeles area end up having their lives become intertwined in some very unusual ways . i 'm not going into much greater detail than that , since it would take forever to explain , and quite frankly , i 'm not willing to spend any more time on it than the 2 hours that i 've already wasted . while it tries very hard to be pulp fiction , 2 days in the valley falls way short . this is quite a condemnation considering the cast includes danny aiello , james spader and jeff daniels . while the story is n't much , and the dialogue and characters rate only marginally better , 2 days in the valley does have a couple of bright spots . james spader 's character , while not much better than the rest , is at least fun to watch in a sick sort of way . and we do get to see a nice cat fight between uber babes teri hatcher and charlize theron ( in her first role ) . you know a movie is n't that good when the highlight is a brawl between two women . even if they are both gorgeous . i will give the writers some credit for the fairly clever ways in which they managed to intersect the lives of this group of characters that would have otherwise never interacted . but marveling at that ingenuity is a far cry from actually enjoying the result . while some of the characters and their respective stories are fairly interesting , they inevitably are brought down as they intersect with the other half of the characters that i really cared absolutely nothing about . if i were to put a number on it , only about half of the story and half the characters in this movie were particularly interesting or otherwise enjoyable to watch . this is the sort of movie that only a huge fan of one or more of the cast members should rent , and even then prepare for disappointment . not even charlize theron being naked would get me to sit through this movie again . or at least not all of it anyway .\",\n          \"when i watch a movie like mike nichols ' what planet are you from ? i ca n't help but feel like everyone is looking at me . it 's as if all the audience is gazing at the back of my head in the darkness , eyes shooting daggers , quietly blaming me for the fact that they paid hard - earned money to spend their time watching this . . . this . . . thing . i shift uncomfortably in my seat . i 'm reminded of how i feel when i see a pair of second- or third - rate celebrities engaging in a teleprompted \\\" funny \\\" conversation to introduce the next blockbuster award . it 's not my fault , i know it 's not my fault , but dammit , someone 's got ta be embarrassed , because it does n't look like anyone on the screen is ready to take the blame . i 'm about to give you a list of names of people who are gon na make a movie together : garry shandling , annette bening , john goodman , greg kinnear , mike nichols . do any of these names make you instantly shudder ? the answer i would have come up with before today is no , this is quite a list of talented individuals we 've got here . granted , john goodman was in the flintstones , and greg kinnear has turned in some less - than - lackluster leading man performances in certified failures like a smile like yours , but even so , they 've got proven power as excellent supporting players . garry shandling has two television classics under his belt , his ingenious little it 's garry shandling 's show and the larry sanders show . and for god 's sake , mike nichols directed the graduate , and annette bening is just walking away from american beauty . so explain this . . . this . . . thing . what planet are you from ? purports to be a comedy exploring the relationship between men and women satirizing the whole pop - psychology mars / venus phenomenon . but what this movie winds up being is a collection of unfunny cringe - inducing moments coupled with uninsightful cringe - inducing moments ; the end result is , unsurprisingly , an unfunny , uninsightful , cringe - inducing , thoroughly icky embarrassment . garry shandling plays an alien from a planet populated by technologically advanced but emotionally vacant males ( they reproduce through cloning , of course ) . his leaders put a select group of males through a series of tests designed to determine which one is most fit to fly off to earth , find a female of the species , and impregnate her . they 're taught how to pretend that they 're listening by nodding and saying \\\" uh - huh , \\\" and how to compliment shoes . imagine my delighted surprise when , oh heavens , all their carefully practiced tactics turn out to fail miserably , producing comedic results ! the lucky winner is fitted with a penis ( since theirs , after generations of disuse have long since shrunk out of existence . . . i will restrain myself from mentioning the implausibility of such a scenario since the population has stopped evolving due to the fact that they 're all just clones . . . whoops , too late . ) the penis , when aroused , tends to make a humming noise . the writers , when frequently strapped for ideas , tend to turn to this as a source of \\\" comedy . \\\" it is n't funny the first time . it is n't funny the eighth time . it is n't funny the eighteenth time . if anything , it made me feel vaguely self - conscious . garry meets up with a coworker at a bank played by greg kinnear , who turns out to be a generic , unlikable scumbag . he 's meant to fill the part of unfortunate role model for shandling 's alien character , but he 's so flatly drawn that even the writers quickly give up and toss him aside . kinnear 's scumbagginess is demonstrated by the fact that he claims other peoples ' work as his own to worm his way into a vice presidents ' position and goes to aa meetings to pick up chicks . wow . what a magnificent bastard . nearly every man in the movie , in fact , is played as the same sort of sex - driven slimeball . when kinnear 's wife walks into the office , there is n't a single guy who does n't trip , bump into a wall , or otherwise pratfall as if they 'd never seen a woman before . the few guys that are n't particularly slimeballs , such as john goodman 's detective character , are simply uncommunicative workaholics . shandling meets up with annette bening , who will inevitably prove to be the love he never knew existed , at one of kinnear 's aa meetings . shandling 's mission is to have a baby , and when he reveals his desires to her , bening instantly falls for him , and the next day . . . they get married . yup . the next day . cuz ya see , it turns out she wants a baby too ! bening 's character perhaps was the most difficult to watch , especially after seeing her come apart at the seams so effectively in american beauty . . . if her character here is supposed to be representing the female of the species as a whole , then woe , i say , to the species . she 's unfathomably insecure , and succumbs so easily to all of shandling 's lines and lies that it borders on tragic . there 's a point where , after thinking she may not be able to bear children , she learns that she is indeed pregnant . when garry comes home after nearly cheating on her , she strolls into the kitchen and sings \\\" high hopes \\\" ( you know , the uplifting ant and the rubber tree plant song ) to deliver the news , and then says to him , \\\" now you ca n't leave me . \\\" we 're supposed to empathize with shandling 's discovery of the feeling of \\\" guilt , \\\" but instead i wanted to weep for bening that she was placing her entire life and soul firmly in the lap of a great big nothing . and eventually , shandling falls in love with her . . . for real , i suppose , though i 'm not sure exactly what prompted it . what 's the message i derive from all this ? men are liars , inherently empty creatures , but if you hang around long enough . . . well , maybe something will click . ha ha . . . ha ? i 'm thankful such broad cynicism is n't frequently allowed to run so rampant . let 's all join hands and pray that the planet these folks are from is not this one . there 's also a subplot involving john goodman as an airline incident investigator that wades in the bog of stupidity . goodman , through a series of astoundingly implausible realizations , puts together the fact that shandling is a being from another world with a magic , vibrating penis . it has all the makings for a subplot of having shandling be discovered , that , thankfully , never comes to the inevitable hackneyed fruition . instead , it just dangles limply on the branch for a while , withers , and falls away . further proof that goodman should just stick to doing coen brothers movies . but let 's not dwell on this any longer , i 've already wasted plenty of your time and my own . let 's move on , forget about what we 've seen here , and get on with our lives . and to help us out , let 's end things on a happy note . . . congratuations go out to annette bening , winner of this week 's \\\" title ! \\\" award , for delivering the awkward line of dialog containing the movie 's name .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# convert the validation dataset to a pandas dataframe\n",
        "val_df = pd.DataFrame(validation_data)\n",
        "val_df.drop(columns=['query', 'query_type'], inplace=True)\n",
        "val_df['evidences'] = val_df['evidences'].astype(str)\n",
        "\n",
        "val_rationales = val_df['evidences']\n",
        "val_reviews = [get_content(validation_data, i) for i in range(val_size)]\n",
        "val_classes = [get_classes(validation_data, i) for i in range(val_size)]\n",
        "\n",
        "print(\"Number of reviews in validation data:\",len(val_reviews))\n",
        "print(\"Max seq length of reviews:\", np.max([len(review.split()) for review in val_reviews]))\n",
        "val_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Q4vG15P9OvtM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "e967031b-f4d7-49f3-c1fa-4e0ca793df53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of reviews in test data: 199\n",
            "Max seq length of reviews: 2122\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  annotation_id  classification  \\\n",
              "0  negR_900.txt               0   \n",
              "1  negR_901.txt               0   \n",
              "2  negR_902.txt               0   \n",
              "3  negR_903.txt               0   \n",
              "4  negR_904.txt               0   \n",
              "\n",
              "                                           evidences  \\\n",
              "0                                 ['i even giggled']   \n",
              "1                                          ['rings']   \n",
              "2  [\"there 're so many things to criticize about ...   \n",
              "3  [\"do n't let this movie fool you into believin...   \n",
              "4  [\"is proof that hollywood does n't have a clue...   \n",
              "\n",
              "                                             content  \n",
              "0  there may not be a critic alive who harbors as...  \n",
              "1  renee zellweger stars as sonia , a young jewis...  \n",
              "2  there 're so many things to criticize about i ...  \n",
              "3  do n't let this movie fool you into believing ...  \n",
              "4  it 's a good thing most animated sci - fi movi...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7ff94d0f-3285-43a4-8792-d621be541c60\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>annotation_id</th>\n",
              "      <th>classification</th>\n",
              "      <th>evidences</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negR_900.txt</td>\n",
              "      <td>0</td>\n",
              "      <td>['i even giggled']</td>\n",
              "      <td>there may not be a critic alive who harbors as...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>negR_901.txt</td>\n",
              "      <td>0</td>\n",
              "      <td>['rings']</td>\n",
              "      <td>renee zellweger stars as sonia , a young jewis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negR_902.txt</td>\n",
              "      <td>0</td>\n",
              "      <td>[\"there 're so many things to criticize about ...</td>\n",
              "      <td>there 're so many things to criticize about i ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negR_903.txt</td>\n",
              "      <td>0</td>\n",
              "      <td>[\"do n't let this movie fool you into believin...</td>\n",
              "      <td>do n't let this movie fool you into believing ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negR_904.txt</td>\n",
              "      <td>0</td>\n",
              "      <td>[\"is proof that hollywood does n't have a clue...</td>\n",
              "      <td>it 's a good thing most animated sci - fi movi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ff94d0f-3285-43a4-8792-d621be541c60')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7ff94d0f-3285-43a4-8792-d621be541c60 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7ff94d0f-3285-43a4-8792-d621be541c60');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4814c063-48a9-4156-a8b4-e5dc6e31f717\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4814c063-48a9-4156-a8b4-e5dc6e31f717')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4814c063-48a9-4156-a8b4-e5dc6e31f717 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_df",
              "summary": "{\n  \"name\": \"test_df\",\n  \"rows\": 199,\n  \"fields\": [\n    {\n      \"column\": \"annotation_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 199,\n        \"samples\": [\n          \"negR_982.txt\",\n          \"negR_915.txt\",\n          \"posR_911.txt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"classification\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"evidences\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 198,\n        \"samples\": [\n          \"['but , there is almost no life to original script by michael laughlin and buck henry and this is \\\" t&c \\\\'s \\\" downfall']\",\n          \"['script keeps the show moving with some nice one -']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 199,\n        \"samples\": [\n          \"i remember really enjoying this movie when i saw it years ago . i guess my memory really sucks . there is very , very little that is funny in caddyshack . the laughs are few , and far between , and what there are really are n't that great . caddyshack , as the name implies , more or less centers on one young caddy working at an exclusive country club . michael o'keefe plays said caddy . why they cast this unknown , fairly untalented actor in the lead role is completely beyond me . the movie does n't seem to have a real plot , just a series of scenes that are little more than opportunities for the rest of the cast to mug at the camera . the only real story , if you can call it that , was a subplot involving the mentally disturbed greens keeper , bill murray , who is having his own private little war against a gopher who is ruining the course . most of the marginal laughs come from rodney dangerfield and ted knight mugging and overacting for the camera -- with painfully limited success . bill murray is slightly amusing in places , but fairly wasted . the biggest waste of all is chevy chase , who did n't even crack a smile on my face with his character 's lame zen - like approach to golfing . there are a few decent scenes involving the interaction between dangerfield and knight , but they are far too infrequent to carry the movie . i guess that 's what you get for basing a story around an unknown kid . i 'm not sure what the writers of this thing were thinking of , but i really think it was something far removed from comedy as they were putting pen to paper . nothing about this movie works . it would n't have taken a genius to figure out that this thing was n't going to fly . most of the scenes just could n't possibly be funny . it 's as if the writers where off in their own little brain damaged world . i 'm sure scenes involving chevy chase and his oneness with the golf ball were supposed to be funny . in reality , they were painfully embarrassing to watch . there is a scene at the club pool where all the caddies go wild for the \\\" hot babe \\\" of the movie walking by in her bikini . olive oil would have filled out this swimsuit better than this girl . everything about this movie was just completely implausible as far as the comedy was concerned . maybe if you were drunk out of you mind or high off some sort of illegal narcotic this thing might be funny . but for the rest of us , stay the hell away from caddyshack .\",\n          \"there 's only one presidential election every four years , but it seems like every few months we get another presidential conspiracy movie painted as _ the _ thriller of the year . in 1997 , we 've had absolute power , air force one , shadow conspiracy and murder at 1600 . this one is about as lame duck as old gerald ford , trying to bring us a complex plot of cover - up and intrigue but copping out over and over again with rehashes of action flick standbys . here 's what happens this time . it 's night at the white house . a secretary is having sex with some unidentified guy with a cute butt . the next day she 's dead and hotshot detective wesley snipes is called in . how do we know he 's a hotshot ? we 've seen the traditional action flick opener -- the clever hostage negotiation scene . it 's not so clever this time , consisting of snipes disarming a suicidal ex - government employee holding a gun to his head in the middle of the street . snipes is off to the white house , where he finds the secret service head ( the shiny bald head of daniel benzali ) wo n't cooperate with him at all . in fact , if not for the intervention of national security adviser alan alda , snipes would n't have been allowed in the white house at all . alda helps snipes out further , assigning a sexy secret service agent ( diane lane ) to act as his liaison . .. a very dangerous liaison . well , not really , i just wanted to say that . almost immediately , a suspect is found , an eccentric night janitor seen flirting with the deceased on one of the security videos . snipes does n't buy it , and launches into an independent investigation of his own , one that reveals planted evidence and romantic involvement by the president 's son . snipes ' partner , an always- wisecracking dennis miller , calls him up every once in awhile with more news and lane , who at first does n't believe snipes , eventually and predictably comes around , and risks her ass to break into social security storage and break out some classified information . for the first hour or so , murder at 1600 looks like it could be going somewhere interesting . sure , we have to sit through the lame opening sequence and plenty more lame scenes after that , but the whole murder in the white house thing makes for an interesting premise that is never quite delivered upon . snipes and lane do n't make for a bad action team , but with nothing to work with , they 're just cogs in the bad movie machine . dennis miller might as well not even be in the movie ; they waste his talents more in murder at 1600 than they did in bordello of blood , and that 's saying a lot . when you get to the last half - hour , the movie has descended metaphorically and literally into a wet sewer , busting out the old break - into - the - building underground climax . and when they finally reveal who killed the woman and why , you 'll wish you never sat through this movie at all . the \\\" 1600 \\\" in the movie 's title does n't represent an address , it represents the number of satisfied customers worldwide . serving the world for nearly 1/25th of a century !\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# convert the test dataset to a pandas dataframe\n",
        "test_df = pd.DataFrame(test_data)\n",
        "test_df.drop(columns=['docids','query', 'query_type'], inplace=True)\n",
        "test_df['evidences'] = test_df['evidences'].astype(str)\n",
        "\n",
        "test_rationales = test_df['evidences']\n",
        "test_reviews = [get_content(test_data, i) for i in range(test_size)]\n",
        "test_classes = [get_classes(test_data, i) for i in range(test_size)]\n",
        "\n",
        "print(\"Number of reviews in test data:\",len(test_reviews))\n",
        "print(\"Max seq length of reviews:\", np.max([len(review.split()) for review in test_reviews]))\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbHzxzSlwz1E"
      },
      "source": [
        "# Convert the reviews & rationales to their corresponding Glove embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "\n",
        "def create_glove_dict(sequence, wv, set, embed_dim=50):\n",
        "    \"\"\"\n",
        "    Creates a dictionary mapping words in the vocabulary to their GloVe embeddings.\n",
        "    Words that don't exist are mapped to zero vectors.\n",
        "    \"\"\"\n",
        "    glove_dict = {}\n",
        "    empty_vec = np.zeros(embed_dim, dtype=np.float64)\n",
        "\n",
        "    for word in tqdm(sequence, desc=f\"Building {set} GloVe dictionary\"):\n",
        "        glove_dict[word] = wv[word] if word in wv else empty_vec\n",
        "\n",
        "    return glove_dict\n",
        "\n",
        "def get_w2GloVe(data, glove_dict, set, embed_dim=50, rationale=False):\n",
        "    \"\"\"\n",
        "    Retrieves the GloVe embeddings using the custom-built GloVe dictionary.\n",
        "    Args:\n",
        "        data: List of text reviews.\n",
        "        glove_dict (dict): custom-built GloVe dictionary.\n",
        "        embed_dim (int): Dimensions of GloVe embeddings.\n",
        "    Returns:\n",
        "        torch.Tensor: Padded tensor of GloVe embeddings to maintain uniform length.\n",
        "    \"\"\"\n",
        "    glove_reviews = []\n",
        "\n",
        "    if rationale:\n",
        "        for review in tqdm(data, desc=f\"Retrieving {set} GloVe Word Embeddings\"):\n",
        "            tokens = \",\".join(review)\n",
        "            words = tokens.split()\n",
        "            embeddings = [glove_dict.get(word, np.zeros(embed_dim)) for word in words]\n",
        "            glove_reviews.append(torch.tensor(embeddings, dtype=torch.float))\n",
        "    else:\n",
        "        for review in tqdm(data, desc=f\"Retrieving {set} GloVe Word Embeddings\"):\n",
        "            words = review.split()\n",
        "            embeddings = [glove_dict.get(word, np.zeros(embed_dim)) for word in words]\n",
        "            glove_reviews.append(torch.tensor(embeddings, dtype=torch.float))\n",
        "\n",
        "    return pad_sequence(glove_reviews, batch_first=True)"
      ],
      "metadata": {
        "id": "Wyf_NLgsB6mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"----------------------------------------------------------------------------------------\\nProcessing Reviews\\n----------------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "# Extract vocabulary(distinct words) from training, validation, and test data\n",
        "train_vocab = set(word for review in train_df['content'] for word in review.split())\n",
        "val_vocab = set(word for review in val_df['content'] for word in review.split())\n",
        "test_vocab = set(word for review in test_df['content'] for word in review.split())\n",
        "\n",
        "# Build the GloVe dictionary for the reviews\n",
        "glove_dict = create_glove_dict(train_vocab, wv, \"training\")\n",
        "glove_dict.update(create_glove_dict(val_vocab, wv, \"validation\"))\n",
        "glove_dict.update(create_glove_dict(test_vocab, wv, \"test\"))\n",
        "\n",
        "# Convert reviews to glove embeddings\n",
        "train_review_gloves = get_w2GloVe(train_df['content'], glove_dict, \"training\")\n",
        "val_review_gloves = get_w2GloVe(val_df['content'], glove_dict, \"validation\")\n",
        "test_review_gloves = get_w2GloVe(test_df['content'], glove_dict, \"test\")\n",
        "\n",
        "print(f\"----------------------------------------------------------------------------------------\\nProcessing Rationales\\n----------------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "#Extract vocabulary(distinct words) from training, validation, and test data for the rationales\n",
        "train_rationale_vocab = set(word for rationale in train_rationales for word in rationale.split())\n",
        "val_rationale_vocab = set(word for rationale in val_rationales for word in rationale.split())\n",
        "test_rationale_vocab = set(word for rationale in test_rationales for word in rationale.split())\n",
        "\n",
        "# Build the GloVe dictionary for the rationales\n",
        "dict_rat = create_glove_dict(train_rationale_vocab, wv, \"training\")\n",
        "dict_rat.update(create_glove_dict(val_rationale_vocab, wv, \"validation\"))\n",
        "dict_rat.update(create_glove_dict(test_rationale_vocab, wv, \"test\"))\n",
        "\n",
        "# Convert rationales to glove embeddings\n",
        "train_rationale_gloves = get_w2GloVe(train_rationales, glove_dict, \"training\", rationale=True)\n",
        "val_rationale_gloves = get_w2GloVe(val_rationales, glove_dict, \"validation\", rationale=True)\n",
        "test_rationale_gloves = get_w2GloVe(test_rationales, glove_dict, \"test\", rationale=True)"
      ],
      "metadata": {
        "id": "Fdo_KLsqh2GR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the GloVe embeddings to local files for faster Access."
      ],
      "metadata": {
        "id": "d4-82zzBMiq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "\n",
        "with open(\"train_reviews.pkl\", \"wb\") as f:\n",
        "    pickle.dump(train_review_gloves, f)\n",
        "\n",
        "with open(\"val_reviews.pkl\", \"wb\") as f:\n",
        "    pickle.dump(val_review_gloves, f)\n",
        "\n",
        "with open(\"test_reviews.pkl\", \"wb\") as f:\n",
        "    pickle.dump(test_review_gloves, f)\n",
        "\n",
        "with open(\"train_rationales.pkl\", \"wb\") as f:\n",
        "    pickle.dump(train_rationale_gloves, f)\n",
        "\n",
        "with open(\"val_rationales.pkl\", \"wb\") as f:\n",
        "    pickle.dump(val_rationale_gloves, f)\n",
        "\n",
        "with open(\"test_rationales.pkl\", \"wb\") as f:\n",
        "    pickle.dump(test_rationale_gloves, f)\n"
      ],
      "metadata": {
        "id": "52lzvzUTMpWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract the GloVe embeddings created above and a create a copy before batching them."
      ],
      "metadata": {
        "id": "WWFvK9oJiIyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"train_reviews.pkl\", \"rb\") as f:\n",
        "    train_in = pickle.load(f)\n",
        "\n",
        "with open(\"train_rationales.pkl\", \"rb\") as f:\n",
        "    train_ev = pickle.load(f)\n",
        "\n",
        "with open(\"val_reviews.pkl\", \"rb\") as f:\n",
        "    val_in = pickle.load(f)\n",
        "\n",
        "with open(\"val_rationales.pkl\", \"rb\") as f:\n",
        "    val_ev = pickle.load(f)\n",
        "\n",
        "with open(\"test_reviews.pkl\", \"rb\") as f:\n",
        "    test_in = pickle.load(f)\n",
        "\n",
        "with open(\"test_rationales.pkl\", \"rb\") as f:\n",
        "    test_ev = pickle.load(f)"
      ],
      "metadata": {
        "id": "fzx5vWVhiKvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSzDGF43ytnN"
      },
      "source": [
        "# Convert the training data to batches using DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsgVwakDytnO"
      },
      "outputs": [],
      "source": [
        "train_inputs = TensorDataset(train_in, train_ev)\n",
        "val_inputs = TensorDataset(val_in, val_ev)\n",
        "test_inputs = TensorDataset(test_in, test_ev)\n",
        "\n",
        "train_loader = DataLoader(train_inputs, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_inputs, batch_size=25, shuffle=False)\n",
        "test_loader = DataLoader(test_inputs, batch_size=25, shuffle=False, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPQy0BjBytnO"
      },
      "source": [
        "# Convolutional Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smXuMplUytnO"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional, Tuple, Union\n",
        "from torch import Tensor\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: Tuple[int, ...],\n",
        "        pool_size: Tuple[int, ...],\n",
        "        stride: Tuple[int, ...],\n",
        "        padding: Tuple[int, ...],\n",
        "        bias: bool\n",
        "    ) -> None:\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.pool_size = pool_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.bias = bias\n",
        "\n",
        "        super(MyModel, self).__init__()\n",
        "        self.ReLU_Activation = nn.ReLU()\n",
        "        self.convolutionLayer = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
        "        self.poolingLayer = nn.MaxPool1d(kernel_size=pool_size, stride=stride, padding=padding)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        X: Tensor,\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\n",
        "        Embedding Layer X --Theta--> E\n",
        "        Input: X (Tensor)\n",
        "        Output: E (Tensor)\n",
        "        Parameter: Theta (optional)\n",
        "        \"\"\"\n",
        "\n",
        "        X = X.squeeze(0)\n",
        "        X_shape = list(X.size())\n",
        "        #print(X_shape)\n",
        "        #print(X)\n",
        "        self.embeddingLayer = nn.Linear(2809, self.in_channels, bias=False)\n",
        "        self.embeddingLayer.weight.data = torch.randn((3,11),dtype=torch.float)\n",
        "        E = self.embeddingLayer(X)\n",
        "        #print(E)\n",
        "\n",
        "        E_shape = list(E.size())\n",
        "        if len(E_shape) < 3:\n",
        "            E = E.unsqueeze(0)\n",
        "        E = E.permute(0, 2, 1)\n",
        "\n",
        "        \"\"\"\n",
        "        Convolution Layer E --W--> H\n",
        "        Input: E (Tensor)\n",
        "        Output: H (Tensor)\n",
        "        Parameter: W (optional)\n",
        "        \"\"\"\n",
        "        #self.convolutionLayer.weight.data = W_torch.permute(2, 0, 1)\n",
        "        H = self.convolutionLayer(E)\n",
        "        #print(H)\n",
        "        H = H.squeeze(0)\n",
        "        H = H.permute(1, 0)\n",
        "\n",
        "        \"\"\"\n",
        "        Dectector Layer H --Psi--> D with ReLU\n",
        "        Input: H (Tensor)\n",
        "        Output: D (Tensor)\n",
        "        Parameter: Psi (optional)\n",
        "        \"\"\"\n",
        "        myPsi_torch = torch.randn((2,1), dtype=torch.float)\n",
        "        D = torch.einsum('ij,jk->ik', H, myPsi_torch)\n",
        "        self.ReLU_Activation = nn.ReLU()\n",
        "        D = self.ReLU_Activation(D)\n",
        "        #print(D)\n",
        "        D = D.permute(1, 0)\n",
        "\n",
        "        \"\"\"\n",
        "        Pooling Layer D --MaxPool--> Y\n",
        "        Input: D (Tensor)\n",
        "        output: Y (Tensor)\n",
        "        Parameter: Pooling_window (optional)\n",
        "        \"\"\"\n",
        "        P = self.poolingLayer(D)\n",
        "        Y_hat = torch.zeros(X_shape[0])\n",
        "        D_shape = list(D.size())\n",
        "        out_size =  D_shape[1] - self.pool_size // self.stride + 1\n",
        "        Y_hat[:out_size] = P\n",
        "\n",
        "        return Y_hat\n",
        "\n",
        "model = MyModel(3, 2, 3, 2, 1, 0, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOONmTg-ytnP"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IXthmm0ytnP"
      },
      "outputs": [],
      "source": [
        "def train(xdata, ydata):\n",
        "    '''Train the neural model with the given training data'''\n",
        "\n",
        "    #Construct the loss function\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    # Construct the optimizer (Stochastic Gradient Descent in this case)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)  # lr is learning rate\n",
        "\n",
        "    # Gradient Descent\n",
        "    for epoch in range(201):\n",
        "        # Forward pass: Compute predicted y by passing x to the model\n",
        "        Y_pred = model(xdata)\n",
        "\n",
        "        # Compute and print loss\n",
        "        loss = criterion(Y_pred, ydata)\n",
        "\n",
        "        if epoch >0 and epoch % 40 == 0:\n",
        "            print('epoch: ', epoch,' loss: ', loss.item())\n",
        "\n",
        "        # Zero gradients, perform a backward pass, and update the weights.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # perform a backward pass (backpropagation)\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "\n",
        "    return Y_pred\n",
        "\n",
        "batch_input = torch.zeros((2809, 11), dtype=torch.float)\n",
        "predList = []\n",
        "batch_outputs = torch.zeros(5, dtype=torch.float)\n",
        "k = 0\n",
        "for batch in dataLoader:\n",
        "    batch_input = batch[0]\n",
        "    y_star = Y_star[k]\n",
        "    k += 1\n",
        "    print(\"-----------------------------------------------------------------------\")\n",
        "    print(\"Training batch:\", k)\n",
        "    print(\"-----------------------------------------------------------------------\")\n",
        "    Y_pred = train(batch_input, y_star)\n",
        "    #print(Y_pred.size())\n",
        "    predList.append(Y_pred[:5])\n",
        "    if k == 5:\n",
        "        break\n",
        "batch_outputs = torch.stack(predList)\n",
        "#print(batch_outputs.size())\n",
        "print(\"======================================================================\")\n",
        "mse=torch.mean( (batch_outputs-Y_star[:5])**2)  # detach takes the tensor out of the network\n",
        "print(\"MSE for ground truth y_star\", mse)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "accelerator": "GPU"
>>>>>>> 32ab414733aa1535dcb0704c2e6a5a7ad20d139d
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6L2Do3b_ytm6"
   },
   "source": [
    "# Final Project CS852 - Foundations of Neural Networks (FALL 2024)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICkcYT3Iytm9"
   },
   "source": [
    "Devin Borchard and Nikhil Kadapala\n",
    "\n",
    "Department of Computer Science, University of New Hampshire\n",
    "\n",
    "ERASER datasets: https://www.eraserbenchmark.com/\n",
    "\n",
    "ERASER paper: https://arxiv.org/pdf/1911.03429\n",
    "\n",
    "LIME paper: https://arxiv.org/pdf/1602.04938"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sr8uEiG3ytm9"
   },
   "source": [
    "# Notebook setup and PyTorch Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "LJ5b9esmytm-"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# uncomment one of these versions (depending on whether you are on a computer with a CPU or not)\n",
    "\n",
    "# GPU version\n",
    "# !conda install --yes --prefix {sys.prefix} pytorch torchvision cudatoolkit=10.2 -c pytorch\n",
    "\n",
    "# Just CPU\n",
    "# !conda install --yes --prefix {sys.prefix} pytorch torchvision cpuonly -c pytorch\n",
    "\n",
    "# install `Einops` for einstein-style tensor manipulation in pytorch\n",
    "# Also see https://github.com/arogozhnikov/einops\n",
    "# !conda install --yes --prefix {sys.prefix} einops  -c conda-forge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XSKM3pNDytnA",
    "outputId": "7138dbf0-2936-49a4-b49e-a0b5d580ff9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0590, 0.6139, 0.1171],\n",
      "        [0.2189, 0.3295, 0.6118],\n",
      "        [0.3489, 0.3336, 0.8039],\n",
      "        [0.3434, 0.7878, 0.6468],\n",
      "        [0.2938, 0.8260, 0.7635]])\n",
      "GPU/CUDA available?  False\n",
      "Torch version 2.5.1\n"
     ]
    }
   ],
   "source": [
    "# torch test\n",
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "\n",
    "print(\"GPU/CUDA available? \", torch.cuda.is_available())\n",
    "\n",
    "print(\"Torch version\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIw0tI1IytnB"
   },
   "source": [
    "# **Extracting Traning, Validation, and Test Data**\n",
    "# Parse the data files to extract the reviews, classifications and annotations for each split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIrDTApeytnC"
   },
   "source": [
    "There are three files:\n",
    "- train.jsonl: containts 1600 training examples\n",
    "- val.jsonl: contains 200 validation examples\n",
    "- test.json: contains 199 test examples\n",
    "\n",
    "Each example includes:\n",
    "- annotation_id: a unique id for an example of the form negR_000 for negative examples and posR_000 for positive examples.\n",
    "- evidences: a list of rationales(specific parts of the review) given by humans that most influenced their classification decision.\n",
    "- classification: the class of the example\n",
    "\n",
    "The annotation_id of each example is the name of the file for the input text data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "pVM3qH6FytnD",
    "outputId": "f003ef8e-9081-43b4-8b87-6a02d0501d1a"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_data(file_path):\n",
    "    data = []                                               # Initialize an empty list to store the dictionaries\n",
    "\n",
    "    with open(file_path, 'r') as file:                      # Open the .jsonl file and read it line by line\n",
    "        for line in file:\n",
    "            annotation = json.loads(line)                   # Parse each line as JSON and append it to the list\n",
    "            id = annotation[\"annotation_id\"]\n",
    "            annotation[\"classification\"] = 1 if annotation['classification'] == \"POS\" else 0\n",
    "\n",
    "            with open(f\"./movies/docs/{id}\", 'r') as file:  # open the file named by annotation_id to extract the review text\n",
    "                content = file.read()\n",
    "                annotation['content'] = content.replace('\\n', ' ')\n",
    "                data.append(annotation)\n",
    "    return data\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "train_file_path = './movies/train.jsonl'\n",
    "val_file_path = './movies/val.jsonl'\n",
    "test_file_path = './movies/test.jsonl'\n",
    "\n",
    "train_data = parse_data(train_file_path)\n",
    "validation_data = parse_data(val_file_path)\n",
    "test_data = parse_data(test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bijy4N-HytnD"
   },
   "source": [
    "# Functions to extract reviews, classifications, and annotations\n",
    "  Define a function\n",
    "\n",
    "  i) to retrieve an example and print the relevant information.\n",
    "\n",
    "  ii) to retrieve the content of the example review text\n",
    "\n",
    "  iii) to retrieve the classifications of the examples\n",
    "  \n",
    "  iv) to retrieve the annotations provided to support the classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "hKGZxYzTytnE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: 1600 training examples\n",
      "               200 validation examples\n",
      "               199 test examples\n",
      "\n",
      "Retrieving Training Example [506].................\n",
      "\n",
      "Review content:\n",
      "this film is extraordinarily horrendous and i 'm not going to waste any more words on it .\n",
      "\n",
      "---------------------------- \n",
      "| Sentiment class: 0 - NEG | \n",
      "----------------------------\n",
      "\n",
      "Human rationales / Supporting Evidence:\n",
      "     -  extraordinarily horrendous\n"
     ]
    }
   ],
   "source": [
    "def print_example(data, index, print_content=True, print_classification=True, print_rationales=True ):\n",
    "    print(f'Retrieving Training Example [{index}].................\\n')\n",
    "    item = data[index]\n",
    "    classification = item['classification']\n",
    "    evidences = item['evidences']\n",
    "    content = item['content']\n",
    "    if print_content: print(f'Review content:\\n{content}\\n')\n",
    "    if print_classification: print('----------------------------',\n",
    "                                   '\\n| Sentiment class:',\n",
    "                                   classification,\n",
    "                                   (\"- NEG\" if not classification else \"- POS\"),\n",
    "                                   '|', '\\n----------------------------')\n",
    "    if print_rationales:\n",
    "        print('\\nHuman rationales / Supporting Evidence:')\n",
    "        for evidence in evidences:\n",
    "            print('     - ', evidence[0]['text'])\n",
    "\n",
    "def get_content(data, index):\n",
    "    item = data[index]\n",
    "    content = item['content']\n",
    "    return content\n",
    "\n",
    "def get_classes(data, index):\n",
    "    item = data[index]\n",
    "    classification = item['classification']\n",
    "    return classification\n",
    "\n",
    "def get_annotations(data, index):\n",
    "    item = data[index]\n",
    "    content = item['evidences']\n",
    "    annotations = [evidence[0]['text'] for evidence in content]\n",
    "    return annotations\n",
    "\n",
    "train_size = len(train_data)\n",
    "val_size = len(validation_data)\n",
    "test_size = len(test_data)\n",
    "\n",
    "print(f'Dataset split: {train_size} training examples')\n",
    "print(f'               {val_size} validation examples')\n",
    "print(f'               {test_size} test examples\\n')\n",
    "\n",
    "print_example(train_data, 506)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLQCckZ-wz1C"
   },
   "source": [
    "# Extraction of the rationales from the evidences metadata of each human annotation of reviews.\n",
    "\n",
    "Each annotation of the review is not the highlighted text/rationale itself but also contains metadata of the text. Use the function defined in the above cell to extract just the text and replace the evidences dictionary of the training, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "-5ulEzugwz1C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['extraordinarily horrendous']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_data)):\n",
    "    train_data[i]['evidences'] = get_annotations(train_data, i)\n",
    "\n",
    "for i in range(len(validation_data)):\n",
    "    validation_data[i]['evidences'] = get_annotations(validation_data, i)\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    test_data[i]['evidences'] = get_annotations(test_data, i)\n",
    "\n",
    "print(train_data[506]['evidences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vc2rC9QTytnK"
   },
   "source": [
    "# Pre-trianed GloVe Embeddings of Training Examples\n",
    "Download the pretrained GloVe Embeddings of desired dimensions using gensim downlader.\n",
    "\n",
    "Save downloaded embeddings to a local file to avoid re-downloading when the kernel or notebook is restarted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "KWyAJ5D_ytnL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    ONLY if you get an error after `import gensim`: update your smart_open liberary\\n    #!conda install --yes --prefix {sys.prefix} smart_open\\n    restart your notebook\\n    see if `import gensim` works now\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Install gensim, to use word2vec word embeddings\n",
    "    Install gensim (for pre-trained word embeddings)\n",
    "    #!conda install --yes --prefix {sys.prefix} gensim\n",
    "\"\"\"\n",
    "#import gensim\n",
    "#import gensim.downloader\n",
    "\n",
    "\"\"\"\n",
    "    ONLY if you get an error after `import gensim`: update your smart_open liberary\n",
    "    #!conda install --yes --prefix {sys.prefix} smart_open\n",
    "    restart your notebook\n",
    "    see if `import gensim` works now\n",
    "\"\"\"\n",
    "#wv = gensim.downloader.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "#import pickle\n",
    "\n",
    "#with open(\"glove_embeddings.pkl\", \"wb\") as f:\n",
    "    #pickle.dump(wv, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "QqA5DXEEwz1D"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.20356 , -0.8707  , -0.19172 ,  0.73862 ,  0.18494 ,  0.14926 ,\n",
       "        0.48079 , -0.21633 ,  0.72753 , -0.36912 ,  0.13397 , -0.1143  ,\n",
       "       -0.18075 , -0.64683 , -0.18484 ,  0.83575 ,  0.48179 ,  0.76026 ,\n",
       "       -0.50381 ,  0.80743 ,  1.2195  ,  0.3459  ,  0.22185 ,  0.31335 ,\n",
       "        1.2066  , -1.8441  ,  0.14064 , -0.99715 , -1.1402  ,  0.32342 ,\n",
       "        3.2128  ,  0.42708 ,  0.19504 ,  0.80113 ,  0.38555 , -0.12568 ,\n",
       "       -0.26533 ,  0.055264, -1.1557  ,  0.16836 , -0.82228 ,  0.20394 ,\n",
       "        0.089235, -0.60125 , -0.032878,  1.3735  , -0.51661 ,  0.29611 ,\n",
       "        0.23951 , -1.3801  ], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"glove_embeddings.pkl\", \"rb\") as f:\n",
    "    wv = pickle.load(f)\n",
    "\n",
    "# lookup the word vector for a word \"india\"\n",
    "wv['india']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "IFRN-vp2ytnM"
   },
   "outputs": [],
   "source": [
    "# downsampled embedding and zero vector for unknown words\n",
    "# note the following code assums the the word embedding dimensions are dividible by 5\n",
    "\n",
    "import einops # type: ignore\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import types\n",
    "\n",
    "def glove_embed(word:str, target_dim)->np.array:\n",
    "    '''Looks up word in embedding (downsampled to five dimensions), pads with beginning of embedding.\n",
    "       Returns zero vector for unknown words.\n",
    "    '''\n",
    "    # these parameters work for 50-dim glove embeddings (adjust for other embeddings)\n",
    "    sampled_dim = 5\n",
    "    sample_batches = 10\n",
    "\n",
    "    empty_vec=np.zeros(target_dim)\n",
    "    if word in wv:\n",
    "        w2v = wv[word] # lookup 50 dim vector\n",
    "        a=einops.reduce(w2v,'(d seg)-> d', \"sum\", seg=sample_batches)  # downsample\n",
    "        b=w2v[0:target_dim-sampled_dim]\n",
    "        return np.hstack([a,b])\n",
    "    else:\n",
    "        return empty_vec\n",
    "\n",
    "def glove_embed_sequences(sequence, target_dim):\n",
    "\n",
    "    if isinstance(sequence, list):\n",
    "        if len(sequence) == 0:\n",
    "            empty_seq = np.zeros(target_dim)\n",
    "            gloveTensor =  torch.tensor(empty_seq, dtype=torch.float)\n",
    "        else:\n",
    "            tokens = \",\".join(sequence)\n",
    "            words = tokens.split()\n",
    "            gloveTensor = torch.stack([torch.tensor(glove_embed(word, target_dim), dtype=torch.float) for word in words])\n",
    "    else:\n",
    "        tokens = sequence.split()\n",
    "        gloveTensor = torch.stack([torch.tensor(glove_embed(token, target_dim), dtype=torch.float) for token in tokens])\n",
    "\n",
    "    return gloveTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YFDJY6pMdHx"
   },
   "source": [
    "# Extract reviews, classifications, and rationales from the train, validation, and test datasets to convert them to Glove embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "XoZiHbVxytnN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews in training data: 1600\n",
      "Max seq length of reviews: 2809\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_id</th>\n",
       "      <th>classification</th>\n",
       "      <th>evidences</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negR_000.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['mind - fuck movie', 'the sad part is', 'down...</td>\n",
       "      <td>plot : two teen couples go to a church party ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negR_001.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"it 's pretty much a sunken ship\", 'sutherlan...</td>\n",
       "      <td>the happy bastard 's quick movie review damn t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negR_002.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['the characters and acting is nothing spectac...</td>\n",
       "      <td>it is movies like these that make a jaded movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negR_003.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['dead on arrival', 'the characters stink', 's...</td>\n",
       "      <td>\" quest for camelot \" is warner bros . ' first...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negR_004.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['it is highly derivative and somewhat boring'...</td>\n",
       "      <td>synopsis : a mentally unstable man undergoing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotation_id  classification  \\\n",
       "0  negR_000.txt               0   \n",
       "1  negR_001.txt               0   \n",
       "2  negR_002.txt               0   \n",
       "3  negR_003.txt               0   \n",
       "4  negR_004.txt               0   \n",
       "\n",
       "                                           evidences  \\\n",
       "0  ['mind - fuck movie', 'the sad part is', 'down...   \n",
       "1  [\"it 's pretty much a sunken ship\", 'sutherlan...   \n",
       "2  ['the characters and acting is nothing spectac...   \n",
       "3  ['dead on arrival', 'the characters stink', 's...   \n",
       "4  ['it is highly derivative and somewhat boring'...   \n",
       "\n",
       "                                             content  \n",
       "0  plot : two teen couples go to a church party ,...  \n",
       "1  the happy bastard 's quick movie review damn t...  \n",
       "2  it is movies like these that make a jaded movi...  \n",
       "3  \" quest for camelot \" is warner bros . ' first...  \n",
       "4  synopsis : a mentally unstable man undergoing ...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the training dataset to a pandas dataframe\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_df.drop(columns=['query', 'query_type'], inplace=True)\n",
    "train_df['evidences'] = train_df['evidences'].astype(str)\n",
    "\n",
    "train_rationales = train_df['evidences']\n",
    "train_reviews = [get_content(train_data, i) for i in range(train_size)]\n",
    "train_classes = [get_classes(train_data, i) for i in range(train_size)]\n",
    "\n",
    "print(\"Number of reviews in training data:\",len(train_reviews))\n",
    "print(\"Max seq length of reviews:\", np.max([len(review.split()) for review in train_reviews]))\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "YUXL-ZR0Pujb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews in validation data: 200\n",
      "Max seq length of reviews: 1880\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_id</th>\n",
       "      <th>classification</th>\n",
       "      <th>evidences</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negR_800.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['definitely the cinematic equivalent of a sle...</td>\n",
       "      <td>there were four movies that earned jamie lee c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negR_801.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['overacts his psycho routine', 'deteriorates ...</td>\n",
       "      <td>according to hitchcock and various other filmm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negR_802.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['so dull and pedestrian and nonsensical', 'bo...</td>\n",
       "      <td>if you 've been following william fichtner 's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negR_803.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['takes the easy route out', 'most hampered no...</td>\n",
       "      <td>note : some may consider portions of the follo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negR_804.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['poor choices', \"it 's downright depressing\",...</td>\n",
       "      <td>for his directoral debut , gary oldman chose a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotation_id  classification  \\\n",
       "0  negR_800.txt               0   \n",
       "1  negR_801.txt               0   \n",
       "2  negR_802.txt               0   \n",
       "3  negR_803.txt               0   \n",
       "4  negR_804.txt               0   \n",
       "\n",
       "                                           evidences  \\\n",
       "0  ['definitely the cinematic equivalent of a sle...   \n",
       "1  ['overacts his psycho routine', 'deteriorates ...   \n",
       "2  ['so dull and pedestrian and nonsensical', 'bo...   \n",
       "3  ['takes the easy route out', 'most hampered no...   \n",
       "4  ['poor choices', \"it 's downright depressing\",...   \n",
       "\n",
       "                                             content  \n",
       "0  there were four movies that earned jamie lee c...  \n",
       "1  according to hitchcock and various other filmm...  \n",
       "2  if you 've been following william fichtner 's ...  \n",
       "3  note : some may consider portions of the follo...  \n",
       "4  for his directoral debut , gary oldman chose a...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the validation dataset to a pandas dataframe\n",
    "val_df = pd.DataFrame(validation_data)\n",
    "val_df.drop(columns=['query', 'query_type'], inplace=True)\n",
    "val_df['evidences'] = val_df['evidences'].astype(str)\n",
    "\n",
    "val_rationales = val_df['evidences']\n",
    "val_reviews = [get_content(validation_data, i) for i in range(val_size)]\n",
    "val_classes = [get_classes(validation_data, i) for i in range(val_size)]\n",
    "\n",
    "print(\"Number of reviews in validation data:\",len(val_reviews))\n",
    "print(\"Max seq length of reviews:\", np.max([len(review.split()) for review in val_reviews]))\n",
    "val_df.to_csv('val_data.csv', index=False)\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Q4vG15P9OvtM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews in test data: 199\n",
      "Max seq length of reviews: 2122\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_id</th>\n",
       "      <th>classification</th>\n",
       "      <th>evidences</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negR_900.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['i even giggled']</td>\n",
       "      <td>there may not be a critic alive who harbors as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negR_901.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['rings']</td>\n",
       "      <td>renee zellweger stars as sonia , a young jewis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negR_902.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"there 're so many things to criticize about ...</td>\n",
       "      <td>there 're so many things to criticize about i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negR_903.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"do n't let this movie fool you into believin...</td>\n",
       "      <td>do n't let this movie fool you into believing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negR_904.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"is proof that hollywood does n't have a clue...</td>\n",
       "      <td>it 's a good thing most animated sci - fi movi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotation_id  classification  \\\n",
       "0  negR_900.txt               0   \n",
       "1  negR_901.txt               0   \n",
       "2  negR_902.txt               0   \n",
       "3  negR_903.txt               0   \n",
       "4  negR_904.txt               0   \n",
       "\n",
       "                                           evidences  \\\n",
       "0                                 ['i even giggled']   \n",
       "1                                          ['rings']   \n",
       "2  [\"there 're so many things to criticize about ...   \n",
       "3  [\"do n't let this movie fool you into believin...   \n",
       "4  [\"is proof that hollywood does n't have a clue...   \n",
       "\n",
       "                                             content  \n",
       "0  there may not be a critic alive who harbors as...  \n",
       "1  renee zellweger stars as sonia , a young jewis...  \n",
       "2  there 're so many things to criticize about i ...  \n",
       "3  do n't let this movie fool you into believing ...  \n",
       "4  it 's a good thing most animated sci - fi movi...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the test dataset to a pandas dataframe\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_df.drop(columns=['docids','query', 'query_type'], inplace=True)\n",
    "test_df['evidences'] = test_df['evidences'].astype(str)\n",
    "\n",
    "test_rationales = test_df['evidences']\n",
    "test_reviews = [get_content(test_data, i) for i in range(test_size)]\n",
    "test_classes = [get_classes(test_data, i) for i in range(test_size)]\n",
    "\n",
    "print(\"Number of reviews in test data:\",len(test_reviews))\n",
    "print(\"Max seq length of reviews:\", np.max([len(review.split()) for review in test_reviews]))\n",
    "test_df.to_csv('test_data.csv', index=False)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_WbCl45wz1E"
   },
   "source": [
    "Extract validation set from the val.jsonl file and create a dataframe for it similar to the training set and save it to a csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbHzxzSlwz1E"
   },
   "source": [
    "# Convert the reviews & rationales to their corresponding Glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_glove_dict(sequence, wv, set, embed_dim=50):\n",
    "    \"\"\"\n",
    "    Creates a dictionary mapping words in the vocabulary to their GloVe embeddings.\n",
    "    Words that don't exist are mapped to zero vectors.\n",
    "    \"\"\"\n",
    "    glove_dict = {}\n",
    "    empty_vec = np.zeros(embed_dim, dtype=np.float64)\n",
    "\n",
    "    for word in tqdm(sequence, desc=f\"Building {set} GloVe dictionary\"):\n",
    "        glove_dict[word] = wv[word] if word in wv else empty_vec\n",
    "\n",
    "    return glove_dict\n",
    "\n",
    "def get_w2GloVe(data, glove_dict, set, embed_dim=50, rationale=False):\n",
    "    \"\"\"\n",
    "    Retrieves the GloVe embeddings using the custom-built GloVe dictionary.\n",
    "    Args:\n",
    "        data: List of text reviews.\n",
    "        glove_dict (dict): custom-built GloVe dictionary.\n",
    "        embed_dim (int): Dimensions of GloVe embeddings.\n",
    "    Returns:\n",
    "        torch.Tensor: Padded tensor of GloVe embeddings to maintain uniform length.\n",
    "    \"\"\"\n",
    "    glove_reviews = []\n",
    "\n",
    "    if rationale:\n",
    "        for review in tqdm(data, desc=f\"Retrieving {set} GloVe Word Embeddings\"):\n",
    "            tokens = \",\".join(review)\n",
    "            words = tokens.split()\n",
    "            embeddings = [glove_dict.get(word, np.zeros(embed_dim)) for word in words]\n",
    "            glove_reviews.append(torch.tensor(embeddings, dtype=torch.float))\n",
    "    else:\n",
    "        for review in tqdm(data, desc=f\"Retrieving {set} GloVe Word Embeddings\"):\n",
    "            words = review.split()\n",
    "            embeddings = [glove_dict.get(word, np.zeros(embed_dim)) for word in words]\n",
    "            glove_reviews.append(torch.tensor(embeddings, dtype=torch.float))\n",
    "\n",
    "    return pad_sequence(glove_reviews, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "Processing Reviews\n",
      "----------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building training GloVe dictionary: 100%|| 36659/36659 [00:00<00:00, 582873.03it/s]\n",
      "Building validation GloVe dictionary: 100%|| 13896/13896 [00:00<00:00, 606967.44it/s]\n",
      "Building test GloVe dictionary: 100%|| 13971/13971 [00:00<00:00, 589784.42it/s]\n",
      "Retrieving training GloVe Word Embeddings: 100%|| 1600/1600 [00:11<00:00, 135.90it/s]\n",
      "Retrieving validation GloVe Word Embeddings: 100%|| 200/200 [00:01<00:00, 109.43it/s]\n",
      "Retrieving test GloVe Word Embeddings: 100%|| 199/199 [00:01<00:00, 112.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "Processing Rationales\n",
      "----------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building training GloVe dictionary: 100%|| 15318/15318 [00:00<00:00, 451480.25it/s]\n",
      "Building validation GloVe dictionary: 100%|| 3335/3335 [00:00<00:00, 476479.34it/s]\n",
      "Building test GloVe dictionary: 100%|| 1664/1664 [00:00<00:00, 335770.32it/s]\n",
      "Retrieving training GloVe Word Embeddings: 100%|| 1600/1600 [00:01<00:00, 1378.41it/s]\n",
      "Retrieving validation GloVe Word Embeddings: 100%|| 200/200 [00:00<00:00, 2215.22it/s]\n",
      "Retrieving test GloVe Word Embeddings: 100%|| 199/199 [00:00<00:00, 4564.29it/s]\n"
     ]
    }
   ],
   "source": [
    "print(f\"----------------------------------------------------------------------------------------\\nProcessing Reviews\\n----------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "# Extract vocabulary(distinct words) from training, validation, and test data\n",
    "train_vocab = set(word for review in train_df['content'] for word in review.split())\n",
    "val_vocab = set(word for review in val_df['content'] for word in review.split())\n",
    "test_vocab = set(word for review in test_df['content'] for word in review.split())\n",
    "\n",
    "# Build the GloVe dictionary for the reviews\n",
    "glove_dict = create_glove_dict(train_vocab, wv, \"training\")\n",
    "glove_dict.update(create_glove_dict(val_vocab, wv, \"validation\"))\n",
    "glove_dict.update(create_glove_dict(test_vocab, wv, \"test\"))\n",
    "\n",
    "# Convert reviews to glove embeddings\n",
    "train_review_gloves = get_w2GloVe(train_df['content'], glove_dict, \"training\")\n",
    "val_review_gloves = get_w2GloVe(val_df['content'], glove_dict, \"validation\")\n",
    "test_review_gloves = get_w2GloVe(test_df['content'], glove_dict, \"test\")\n",
    "\n",
    "print(f\"----------------------------------------------------------------------------------------\\nProcessing Rationales\\n----------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "#Extract vocabulary(distinct words) from training, validation, and test data for the rationales\n",
    "train_rationale_vocab = set(word for rationale in train_rationales for word in rationale.split())\n",
    "val_rationale_vocab = set(word for rationale in val_rationales for word in rationale.split())\n",
    "test_rationale_vocab = set(word for rationale in test_rationales for word in rationale.split())\n",
    "\n",
    "# Build the GloVe dictionary for the rationales\n",
    "dict_rat = create_glove_dict(train_rationale_vocab, wv, \"training\")\n",
    "dict_rat.update(create_glove_dict(val_rationale_vocab, wv, \"validation\"))\n",
    "dict_rat.update(create_glove_dict(test_rationale_vocab, wv, \"test\"))\n",
    "\n",
    "# Convert rationales to glove embeddings\n",
    "train_rationale_gloves = get_w2GloVe(train_rationales, glove_dict, \"training\", rationale=True)\n",
    "val_rationale_gloves = get_w2GloVe(val_rationales, glove_dict, \"validation\", rationale=True)\n",
    "test_rationale_gloves = get_w2GloVe(test_rationales, glove_dict, \"test\", rationale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the GloVe embeddings to local files for faster Access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Pm21m4EZwz1F"
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"train_reviews.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_review_gloves, f)\n",
    "\n",
    "with open(\"val_reviews.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_review_gloves, f)\n",
    "\n",
    "with open(\"test_reviews.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_review_gloves, f)\n",
    "\n",
    "with open(\"train_rationales.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_rationale_gloves, f)\n",
    "\n",
    "with open(\"val_rationales.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_rationale_gloves, f)\n",
    "\n",
    "with open(\"test_rationales.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_rationale_gloves, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the GloVe embeddings created above and a create a copy before batching them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_reviews.pkl\", \"rb\") as f:\n",
    "    train_in = pickle.load(f)\n",
    "\n",
    "with open(\"train_rationales.pkl\", \"rb\") as f:\n",
    "    train_ev = pickle.load(f)\n",
    "\n",
    "with open(\"val_reviews.pkl\", \"rb\") as f:\n",
    "    val_in = pickle.load(f)\n",
    "\n",
    "with open(\"val_rationales.pkl\", \"rb\") as f:\n",
    "    val_ev = pickle.load(f)\n",
    "\n",
    "with open(\"test_reviews.pkl\", \"rb\") as f:\n",
    "    test_in = pickle.load(f)\n",
    "\n",
    "with open(\"test_rationales.pkl\", \"rb\") as f:\n",
    "    test_ev = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSzDGF43ytnN"
   },
   "source": [
    "Convert the training, validation, and test data(GloVe representations) including the rationales to batches using DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "wsgVwakDytnO"
   },
   "outputs": [],
   "source": [
    "train_inputs = TensorDataset(train_in, train_ev)\n",
    "val_inputs = TensorDataset(val_in, val_ev)\n",
    "test_inputs = TensorDataset(test_in, test_ev)\n",
    "\n",
    "train_loader = DataLoader(train_inputs, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_inputs, batch_size=25, shuffle=False)\n",
    "test_loader = DataLoader(test_inputs, batch_size=25, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPQy0BjBytnO"
   },
   "source": [
    "# Convolutional Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "smXuMplUytnO"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_AdaptiveAvgPoolNd.__init__() missing 1 required positional argument: 'output_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 94\u001b[0m\n\u001b[0;32m     90\u001b[0m         Y_hat[:out_size] \u001b[38;5;241m=\u001b[39m P\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Y_hat\n\u001b[1;32m---> 94\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[53], line 28\u001b[0m, in \u001b[0;36mMyModel.__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, pool_size, stride, padding, bias)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvolutionLayer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv1d(in_channels\u001b[38;5;241m=\u001b[39min_channels, out_channels\u001b[38;5;241m=\u001b[39mout_channels, kernel_size\u001b[38;5;241m=\u001b[39mkernel_size, stride\u001b[38;5;241m=\u001b[39mstride, padding\u001b[38;5;241m=\u001b[39mpadding, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoolingLayer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMaxPool1d(kernel_size\u001b[38;5;241m=\u001b[39mpool_size, stride\u001b[38;5;241m=\u001b[39mstride, padding\u001b[38;5;241m=\u001b[39mpadding)\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mAdaptiveAvgPool1d()\n",
      "\u001b[1;31mTypeError\u001b[0m: _AdaptiveAvgPoolNd.__init__() missing 1 required positional argument: 'output_size'"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Tuple[int, ...],\n",
    "        pool_size: Tuple[int, ...],\n",
    "        stride: Tuple[int, ...],\n",
    "        padding: Tuple[int, ...],\n",
    "        bias: bool\n",
    "    ) -> None:\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "\n",
    "        super(MyModel, self).__init__()\n",
    "        self.ReLU_Activation = nn.ReLU()\n",
    "        self.convolutionLayer = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.poolingLayer = nn.MaxPool1d(kernel_size=pool_size, stride=stride, padding=padding)\n",
    "        self.maxpool = nn.AdaptiveAvgPool1d()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        X: Tensor,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Embedding Layer X --Theta--> E\n",
    "        Input: X (Tensor)\n",
    "        Output: E (Tensor)\n",
    "        Parameter: Theta (optional)\n",
    "        \"\"\"\n",
    "\n",
    "        X = X.squeeze(0)\n",
    "        X_shape = list(X.size())\n",
    "        #print(X_shape)\n",
    "        #print(X)\n",
    "        self.embeddingLayer = nn.Linear(2809, self.in_channels, bias=False)\n",
    "        self.embeddingLayer.weight.data = torch.randn((3,11),dtype=torch.float)\n",
    "        E = self.embeddingLayer(X)\n",
    "        #print(E)\n",
    "\n",
    "        E_shape = list(E.size())\n",
    "        if len(E_shape) < 3:\n",
    "            E = E.unsqueeze(0)\n",
    "        E = E.permute(0, 2, 1)\n",
    "\n",
    "        \"\"\"\n",
    "        Convolution Layer E --W--> H\n",
    "        Input: E (Tensor)\n",
    "        Output: H (Tensor)\n",
    "        Parameter: W (optional)\n",
    "        \"\"\"\n",
    "        #self.convolutionLayer.weight.data = W_torch.permute(2, 0, 1)\n",
    "        H = self.convolutionLayer(E)\n",
    "        #print(H)\n",
    "        H = H.squeeze(0)\n",
    "        H = H.permute(1, 0)\n",
    "\n",
    "        \"\"\"\n",
    "        Dectector Layer H --Psi--> D with ReLU\n",
    "        Input: H (Tensor)\n",
    "        Output: D (Tensor)\n",
    "        Parameter: Psi (optional)\n",
    "        \"\"\"\n",
    "        myPsi_torch = torch.randn((2,1), dtype=torch.float)\n",
    "        D = torch.einsum('ij,jk->ik', H, myPsi_torch)\n",
    "        self.ReLU_Activation = nn.ReLU()\n",
    "        D = self.ReLU_Activation(D)\n",
    "        #print(D)\n",
    "        D = D.permute(1, 0)\n",
    "\n",
    "        \"\"\"\n",
    "        Pooling Layer D --MaxPool--> Y\n",
    "        Input: D (Tensor)\n",
    "        output: Y (Tensor)\n",
    "        Parameter: Pooling_window (optional)\n",
    "        \"\"\"\n",
    "        P = self.poolingLayer(D)\n",
    "        Y_hat = torch.zeros(X_shape[0])\n",
    "        D_shape = list(D.size())\n",
    "        out_size =  D_shape[1] - self.pool_size // self.stride + 1\n",
    "        Y_hat[:out_size] = P\n",
    "\n",
    "        return Y_hat\n",
    "\n",
    "model = MyModel(3, 2, 3, 2, 1, 0, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOONmTg-ytnP"
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0IXthmm0ytnP"
   },
   "outputs": [],
   "source": [
    "def train(xdata, ydata):\n",
    "    '''Train the neural model with the given training data'''\n",
    "\n",
    "    #Construct the loss function\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    # Construct the optimizer (Stochastic Gradient Descent in this case)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)  # lr is learning rate\n",
    "\n",
    "    # Gradient Descent\n",
    "    for epoch in range(201):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        Y_pred = model(xdata)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(Y_pred, ydata)\n",
    "\n",
    "        if epoch >0 and epoch % 40 == 0:\n",
    "            print('epoch: ', epoch,' loss: ', loss.item())\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # perform a backward pass (backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    return Y_pred\n",
    "\n",
    "batch_input = torch.zeros((2809, 11), dtype=torch.float)\n",
    "predList = []\n",
    "batch_outputs = torch.zeros(5, dtype=torch.float)\n",
    "k = 0\n",
    "for batch in dataLoader:\n",
    "    batch_input = batch[0]\n",
    "    y_star = Y_star[k]\n",
    "    k += 1\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    print(\"Training batch:\", k)\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    Y_pred = train(batch_input, y_star)\n",
    "    #print(Y_pred.size())\n",
    "    predList.append(Y_pred[:5])\n",
    "    if k == 5:\n",
    "        break\n",
    "batch_outputs = torch.stack(predList)\n",
    "#print(batch_outputs.size())\n",
    "print(\"======================================================================\")\n",
    "mse=torch.mean( (batch_outputs-Y_star[:5])**2)  # detach takes the tensor out of the network\n",
    "print(\"MSE for ground truth y_star\", mse)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
