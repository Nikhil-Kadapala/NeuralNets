{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Nikhil-Kadapala/NeuralNets/blob/main/standardNeuralNets/stdCNN_LIME.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6L2Do3b_ytm6"
   },
   "source": [
    "# Final Project CS852 - Foundations of Neural Networks (FALL 2024)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICkcYT3Iytm9"
   },
   "source": [
    "Devin Borchard and Nikhil Kadapala\n",
    "\n",
    "Department of Computer Science, University of New Hampshire\n",
    "\n",
    "ERASER datasets: https://www.eraserbenchmark.com/\n",
    "\n",
    "ERASER paper: https://arxiv.org/pdf/1911.03429\n",
    "\n",
    "LIME paper: https://arxiv.org/pdf/1602.04938"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sr8uEiG3ytm9"
   },
   "source": [
    "# Notebook setup and PyTorch Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "LJ5b9esmytm-"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# uncomment one of these versions (depending on whether you are on a computer with a CPU or not)\n",
    "\n",
    "# GPU version\n",
    "# !conda install --yes --prefix {sys.prefix} pytorch torchvision cudatoolkit=10.2 -c pytorch\n",
    "\n",
    "# Just CPU\n",
    "# !conda install --yes --prefix {sys.prefix} pytorch torchvision cpuonly -c pytorch\n",
    "\n",
    "# install `Einops` for einstein-style tensor manipulation in pytorch\n",
    "# Also see https://github.com/arogozhnikov/einops\n",
    "# !conda install --yes --prefix {sys.prefix} einops  -c conda-forge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XSKM3pNDytnA",
    "outputId": "7138dbf0-2936-49a4-b49e-a0b5d580ff9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7001, 0.3343, 0.5775],\n",
      "        [0.5755, 0.1432, 0.7815],\n",
      "        [0.1568, 0.9734, 0.4154],\n",
      "        [0.9076, 0.5632, 0.4620],\n",
      "        [0.7791, 0.1319, 0.3354]])\n",
      "GPU/CUDA available?  False\n",
      "Torch version 2.5.1\n"
     ]
    }
   ],
   "source": [
    "# torch test\n",
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "\n",
    "print(\"GPU/CUDA available? \", torch.cuda.is_available())\n",
    "\n",
    "print(\"Torch version\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIw0tI1IytnB"
   },
   "source": [
    "# **Extracting Traning, Validation, and Test Data**\n",
    "# Parse the data files to extract the reviews, classifications and annotations for each split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIrDTApeytnC"
   },
   "source": [
    "There are three files:\n",
    "- train.jsonl: containts 1600 training examples\n",
    "- val.jsonl: contains 200 validation examples\n",
    "- test.json: contains 199 test examples\n",
    "\n",
    "Each example includes:\n",
    "- annotation_id: a unique id for an example of the form negR_000 for negative examples and posR_000 for positive examples.\n",
    "- evidences: a list of rationales(specific parts of the review) given by humans that most influenced their classification decision.\n",
    "- classification: the class of the example\n",
    "\n",
    "The annotation_id of each example is the name of the file for the input text data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "pVM3qH6FytnD",
    "outputId": "f003ef8e-9081-43b4-8b87-6a02d0501d1a"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_data(file_path):\n",
    "    data = []                                               # Initialize an empty list to store the dictionaries\n",
    "\n",
    "    with open(file_path, 'r') as file:                      # Open the .jsonl file and read it line by line\n",
    "        for line in file:\n",
    "            annotation = json.loads(line)                   # Parse each line as JSON and append it to the list\n",
    "            id = annotation[\"annotation_id\"]\n",
    "            annotation[\"classification\"] = 1 if annotation['classification'] == \"POS\" else 0\n",
    "\n",
    "            with open(f\"./movies/docs/{id}\", 'r') as file:  # open the file named by annotation_id to extract the review text\n",
    "                content = file.read()\n",
    "                annotation['content'] = content.replace('\\n', ' ')\n",
    "                data.append(annotation)\n",
    "    return data\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "train_file_path = './movies/train.jsonl'\n",
    "val_file_path = './movies/val.jsonl'\n",
    "test_file_path = './movies/test.jsonl'\n",
    "\n",
    "train_data = parse_data(train_file_path)\n",
    "validation_data = parse_data(val_file_path)\n",
    "test_data = parse_data(test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bijy4N-HytnD"
   },
   "source": [
    "# Functions to extract reviews, classifications, and annotations\n",
    "  Define a function\n",
    "\n",
    "  i) to retrieve an example and print the relevant information.\n",
    "\n",
    "  ii) to retrieve the content of the example review text\n",
    "\n",
    "  iii) to retrieve the classifications of the examples\n",
    "  \n",
    "  iv) to retrieve the annotations provided to support the classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "hKGZxYzTytnE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: 1600 training examples\n",
      "               200 validation examples\n",
      "               199 test examples\n",
      "\n",
      "Retrieving Training Example [506].................\n",
      "\n",
      "Review content:\n",
      "this film is extraordinarily horrendous and i 'm not going to waste any more words on it .\n",
      "\n",
      "---------------------------- \n",
      "| Sentiment class: 0 - NEG | \n",
      "----------------------------\n",
      "\n",
      "Human rationales / Supporting Evidence:\n",
      "     -  extraordinarily horrendous\n"
     ]
    }
   ],
   "source": [
    "def print_example(data, index, print_content=True, print_classification=True, print_rationales=True ):\n",
    "    print(f'Retrieving Training Example [{index}].................\\n')\n",
    "    item = data[index]\n",
    "    classification = item['classification']\n",
    "    evidences = item['evidences']\n",
    "    content = item['content']\n",
    "    if print_content: print(f'Review content:\\n{content}\\n')\n",
    "    if print_classification: print('----------------------------',\n",
    "                                   '\\n| Sentiment class:',\n",
    "                                   classification,\n",
    "                                   (\"- NEG\" if not classification else \"- POS\"),\n",
    "                                   '|', '\\n----------------------------')\n",
    "    if print_rationales:\n",
    "        print('\\nHuman rationales / Supporting Evidence:')\n",
    "        for evidence in evidences:\n",
    "            print('     - ', evidence[0]['text'])\n",
    "\n",
    "def get_content(data, index):\n",
    "    item = data[index]\n",
    "    content = item['content']\n",
    "    return content\n",
    "\n",
    "def get_classes(data, index):\n",
    "    item = data[index]\n",
    "    classification = item['classification']\n",
    "    return classification\n",
    "\n",
    "def get_annotations(data, index):\n",
    "    item = data[index]\n",
    "    content = item['evidences']\n",
    "    annotations = [evidence[0]['text'] for evidence in content]\n",
    "    return annotations\n",
    "\n",
    "train_size = len(train_data)\n",
    "val_size = len(validation_data)\n",
    "test_size = len(test_data)\n",
    "\n",
    "print(f'Dataset split: {train_size} training examples')\n",
    "print(f'               {val_size} validation examples')\n",
    "print(f'               {test_size} test examples\\n')\n",
    "\n",
    "print_example(train_data, 506)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLQCckZ-wz1C"
   },
   "source": [
    "# Extraction of the rationales from the evidences metadata of each human annotation of reviews.\n",
    "\n",
    "Each annotation of the review is not the highlighted text/rationale itself but also contains metadata of the text. Use the function defined in the above cell to extract just the text and replace the evidences dictionary of the training, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "-5ulEzugwz1C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['extraordinarily horrendous']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_data)):\n",
    "    train_data[i]['evidences'] = get_annotations(train_data, i)\n",
    "\n",
    "for i in range(len(validation_data)):\n",
    "    validation_data[i]['evidences'] = get_annotations(validation_data, i)\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    test_data[i]['evidences'] = get_annotations(test_data, i)\n",
    "\n",
    "print(train_data[506]['evidences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vc2rC9QTytnK"
   },
   "source": [
    "# Pre-trianed GloVe Embeddings of Training Examples\n",
    "Download the pretrained GloVe Embeddings of desired dimensions using gensim downlader.\n",
    "\n",
    "Save downloaded embeddings to a local file to avoid re-downloading when the kernel or notebook is restarted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "KWyAJ5D_ytnL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    ONLY if you get an error after `import gensim`: update your smart_open liberary\\n    #!conda install --yes --prefix {sys.prefix} smart_open\\n    restart your notebook\\n    see if `import gensim` works now\\n'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Install gensim, to use word2vec word embeddings\n",
    "    Install gensim (for pre-trained word embeddings)\n",
    "    #!conda install --yes --prefix {sys.prefix} gensim\n",
    "\"\"\"\n",
    "#import gensim\n",
    "#import gensim.downloader\n",
    "\n",
    "\"\"\"\n",
    "    ONLY if you get an error after `import gensim`: update your smart_open liberary\n",
    "    #!conda install --yes --prefix {sys.prefix} smart_open\n",
    "    restart your notebook\n",
    "    see if `import gensim` works now\n",
    "\"\"\"\n",
    "#wv = gensim.downloader.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "#import pickle\n",
    "\n",
    "#with open(\"glove_embeddings.pkl\", \"wb\") as f:\n",
    "    #pickle.dump(wv, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "QqA5DXEEwz1D"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.20356 , -0.8707  , -0.19172 ,  0.73862 ,  0.18494 ,  0.14926 ,\n",
       "        0.48079 , -0.21633 ,  0.72753 , -0.36912 ,  0.13397 , -0.1143  ,\n",
       "       -0.18075 , -0.64683 , -0.18484 ,  0.83575 ,  0.48179 ,  0.76026 ,\n",
       "       -0.50381 ,  0.80743 ,  1.2195  ,  0.3459  ,  0.22185 ,  0.31335 ,\n",
       "        1.2066  , -1.8441  ,  0.14064 , -0.99715 , -1.1402  ,  0.32342 ,\n",
       "        3.2128  ,  0.42708 ,  0.19504 ,  0.80113 ,  0.38555 , -0.12568 ,\n",
       "       -0.26533 ,  0.055264, -1.1557  ,  0.16836 , -0.82228 ,  0.20394 ,\n",
       "        0.089235, -0.60125 , -0.032878,  1.3735  , -0.51661 ,  0.29611 ,\n",
       "        0.23951 , -1.3801  ], dtype=float32)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"glove_embeddings.pkl\", \"rb\") as f:\n",
    "    wv = pickle.load(f)\n",
    "\n",
    "# lookup the word vector for a word \"india\"\n",
    "wv['india']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "IFRN-vp2ytnM"
   },
   "outputs": [],
   "source": [
    "# downsampled embedding and zero vector for unknown words\n",
    "# note the following code assums the the word embedding dimensions are dividible by 5\n",
    "\n",
    "import einops # type: ignore\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import types\n",
    "\n",
    "def glove_embed(word:str, target_dim)->np.array:\n",
    "    '''Looks up word in embedding (downsampled to five dimensions), pads with beginning of embedding.\n",
    "       Returns zero vector for unknown words.\n",
    "    '''\n",
    "    # these parameters work for 50-dim glove embeddings (adjust for other embeddings)\n",
    "    sampled_dim = 5\n",
    "    sample_batches = 10\n",
    "\n",
    "    empty_vec=np.zeros(target_dim)\n",
    "    if word in wv:\n",
    "        w2v = wv[word] # lookup 50 dim vector\n",
    "        a=einops.reduce(w2v,'(d seg)-> d', \"sum\", seg=sample_batches)  # downsample\n",
    "        b=w2v[0:target_dim-sampled_dim]\n",
    "        return np.hstack([a,b])\n",
    "    else:\n",
    "        return empty_vec\n",
    "\n",
    "def glove_embed_sequences(sequence, target_dim):\n",
    "\n",
    "    if isinstance(sequence, list):\n",
    "        if len(sequence) == 0:\n",
    "            empty_seq = np.zeros(target_dim)\n",
    "            gloveTensor =  torch.tensor(empty_seq, dtype=torch.float)\n",
    "        else:\n",
    "            tokens = \",\".join(sequence)\n",
    "            words = tokens.split()\n",
    "            gloveTensor = torch.stack([torch.tensor(glove_embed(word, target_dim), dtype=torch.float) for word in words])\n",
    "    else:\n",
    "        tokens = sequence.split()\n",
    "        gloveTensor = torch.stack([torch.tensor(glove_embed(token, target_dim), dtype=torch.float) for token in tokens])\n",
    "\n",
    "    return gloveTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YFDJY6pMdHx"
   },
   "source": [
    "# Extract reviews, classifications, and rationales from the train, validation, and test datasets to convert them to Glove embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "XoZiHbVxytnN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews in training data: 1600\n",
      "Max seq length of reviews: 2809\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_id</th>\n",
       "      <th>classification</th>\n",
       "      <th>evidences</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negR_000.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['mind - fuck movie', 'the sad part is', 'down...</td>\n",
       "      <td>plot : two teen couples go to a church party ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negR_001.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"it 's pretty much a sunken ship\", 'sutherlan...</td>\n",
       "      <td>the happy bastard 's quick movie review damn t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negR_002.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['the characters and acting is nothing spectac...</td>\n",
       "      <td>it is movies like these that make a jaded movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negR_003.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['dead on arrival', 'the characters stink', 's...</td>\n",
       "      <td>\" quest for camelot \" is warner bros . ' first...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negR_004.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['it is highly derivative and somewhat boring'...</td>\n",
       "      <td>synopsis : a mentally unstable man undergoing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotation_id  classification  \\\n",
       "0  negR_000.txt               0   \n",
       "1  negR_001.txt               0   \n",
       "2  negR_002.txt               0   \n",
       "3  negR_003.txt               0   \n",
       "4  negR_004.txt               0   \n",
       "\n",
       "                                           evidences  \\\n",
       "0  ['mind - fuck movie', 'the sad part is', 'down...   \n",
       "1  [\"it 's pretty much a sunken ship\", 'sutherlan...   \n",
       "2  ['the characters and acting is nothing spectac...   \n",
       "3  ['dead on arrival', 'the characters stink', 's...   \n",
       "4  ['it is highly derivative and somewhat boring'...   \n",
       "\n",
       "                                             content  \n",
       "0  plot : two teen couples go to a church party ,...  \n",
       "1  the happy bastard 's quick movie review damn t...  \n",
       "2  it is movies like these that make a jaded movi...  \n",
       "3  \" quest for camelot \" is warner bros . ' first...  \n",
       "4  synopsis : a mentally unstable man undergoing ...  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the training dataset to a pandas dataframe\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_df.drop(columns=['query', 'query_type'], inplace=True)\n",
    "train_df['evidences'] = train_df['evidences'].astype(str)\n",
    "\n",
    "train_rationales = train_df['evidences']\n",
    "train_reviews = [get_content(train_data, i) for i in range(train_size)]\n",
    "train_classes = torch.tensor([get_classes(train_data, i) for i in range(train_size)], dtype=torch.float)\n",
    "\n",
    "print(\"Number of reviews in training data:\",len(train_reviews))\n",
    "print(\"Max seq length of reviews:\", np.max([len(review.split()) for review in train_reviews]))\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "YUXL-ZR0Pujb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews in validation data: 200\n",
      "Max seq length of reviews: 1880\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_id</th>\n",
       "      <th>classification</th>\n",
       "      <th>evidences</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negR_800.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['definitely the cinematic equivalent of a sle...</td>\n",
       "      <td>there were four movies that earned jamie lee c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negR_801.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['overacts his psycho routine', 'deteriorates ...</td>\n",
       "      <td>according to hitchcock and various other filmm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negR_802.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['so dull and pedestrian and nonsensical', 'bo...</td>\n",
       "      <td>if you 've been following william fichtner 's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negR_803.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['takes the easy route out', 'most hampered no...</td>\n",
       "      <td>note : some may consider portions of the follo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negR_804.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['poor choices', \"it 's downright depressing\",...</td>\n",
       "      <td>for his directoral debut , gary oldman chose a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotation_id  classification  \\\n",
       "0  negR_800.txt               0   \n",
       "1  negR_801.txt               0   \n",
       "2  negR_802.txt               0   \n",
       "3  negR_803.txt               0   \n",
       "4  negR_804.txt               0   \n",
       "\n",
       "                                           evidences  \\\n",
       "0  ['definitely the cinematic equivalent of a sle...   \n",
       "1  ['overacts his psycho routine', 'deteriorates ...   \n",
       "2  ['so dull and pedestrian and nonsensical', 'bo...   \n",
       "3  ['takes the easy route out', 'most hampered no...   \n",
       "4  ['poor choices', \"it 's downright depressing\",...   \n",
       "\n",
       "                                             content  \n",
       "0  there were four movies that earned jamie lee c...  \n",
       "1  according to hitchcock and various other filmm...  \n",
       "2  if you 've been following william fichtner 's ...  \n",
       "3  note : some may consider portions of the follo...  \n",
       "4  for his directoral debut , gary oldman chose a...  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the validation dataset to a pandas dataframe\n",
    "val_df = pd.DataFrame(validation_data)\n",
    "val_df.drop(columns=['query', 'query_type'], inplace=True)\n",
    "val_df['evidences'] = val_df['evidences'].astype(str)\n",
    "\n",
    "val_rationales = val_df['evidences']\n",
    "val_reviews = [get_content(validation_data, i) for i in range(val_size)]\n",
    "val_classes = torch.tensor([get_classes(validation_data, i) for i in range(val_size)], dtype=torch.float)\n",
    "\n",
    "print(\"Number of reviews in validation data:\",len(val_reviews))\n",
    "print(\"Max seq length of reviews:\", np.max([len(review.split()) for review in val_reviews]))\n",
    "val_df.to_csv('val_data.csv', index=False)\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "Q4vG15P9OvtM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews in test data: 199\n",
      "Max seq length of reviews: 2122\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_id</th>\n",
       "      <th>classification</th>\n",
       "      <th>evidences</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negR_900.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['i even giggled']</td>\n",
       "      <td>there may not be a critic alive who harbors as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negR_901.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>['rings']</td>\n",
       "      <td>renee zellweger stars as sonia , a young jewis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negR_902.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"there 're so many things to criticize about ...</td>\n",
       "      <td>there 're so many things to criticize about i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negR_903.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"do n't let this movie fool you into believin...</td>\n",
       "      <td>do n't let this movie fool you into believing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negR_904.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"is proof that hollywood does n't have a clue...</td>\n",
       "      <td>it 's a good thing most animated sci - fi movi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotation_id  classification  \\\n",
       "0  negR_900.txt               0   \n",
       "1  negR_901.txt               0   \n",
       "2  negR_902.txt               0   \n",
       "3  negR_903.txt               0   \n",
       "4  negR_904.txt               0   \n",
       "\n",
       "                                           evidences  \\\n",
       "0                                 ['i even giggled']   \n",
       "1                                          ['rings']   \n",
       "2  [\"there 're so many things to criticize about ...   \n",
       "3  [\"do n't let this movie fool you into believin...   \n",
       "4  [\"is proof that hollywood does n't have a clue...   \n",
       "\n",
       "                                             content  \n",
       "0  there may not be a critic alive who harbors as...  \n",
       "1  renee zellweger stars as sonia , a young jewis...  \n",
       "2  there 're so many things to criticize about i ...  \n",
       "3  do n't let this movie fool you into believing ...  \n",
       "4  it 's a good thing most animated sci - fi movi...  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the test dataset to a pandas dataframe\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_df.drop(columns=['docids','query', 'query_type'], inplace=True)\n",
    "test_df['evidences'] = test_df['evidences'].astype(str)\n",
    "\n",
    "test_rationales = test_df['evidences']\n",
    "test_reviews = [get_content(test_data, i) for i in range(test_size)]\n",
    "test_classes = torch.tensor([get_classes(test_data, i) for i in range(test_size)], dtype=torch.float)\n",
    "\n",
    "print(\"Number of reviews in test data:\",len(test_reviews))\n",
    "print(\"Max seq length of reviews:\", np.max([len(review.split()) for review in test_reviews]))\n",
    "test_df.to_csv('test_data.csv', index=False)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_WbCl45wz1E"
   },
   "source": [
    "Extract validation set from the val.jsonl file and create a dataframe for it similar to the training set and save it to a csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbHzxzSlwz1E"
   },
   "source": [
    "# Convert the reviews & rationales to their corresponding Glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_glove_dict(sequence, wv, set, embed_dim=50):\n",
    "    \"\"\"\n",
    "    Creates a dictionary mapping words in the vocabulary to their GloVe embeddings.\n",
    "    Words that don't exist are mapped to zero vectors.\n",
    "    \"\"\"\n",
    "    glove_dict = {}\n",
    "    empty_vec = np.zeros(embed_dim, dtype=np.float64)\n",
    "\n",
    "    for word in tqdm(sequence, desc=f\"Building {set} GloVe dictionary\"):\n",
    "        glove_dict[word] = wv[word] if word in wv else empty_vec\n",
    "\n",
    "    return glove_dict\n",
    "\n",
    "def get_w2GloVe(data, glove_dict, set, embed_dim=50, rationale=False):\n",
    "    \"\"\"\n",
    "    Retrieves the GloVe embeddings using the custom-built GloVe dictionary.\n",
    "    Args:\n",
    "        data: List of text reviews.\n",
    "        glove_dict (dict): custom-built GloVe dictionary.\n",
    "        embed_dim (int): Dimensions of GloVe embeddings.\n",
    "    Returns:\n",
    "        torch.Tensor: Padded tensor of GloVe embeddings to maintain uniform length.\n",
    "    \"\"\"\n",
    "    glove_reviews = []\n",
    "\n",
    "    if rationale:\n",
    "        for review in tqdm(data, desc=f\"Retrieving {set} GloVe Word Embeddings\"):\n",
    "            tokens = \",\".join(review)\n",
    "            words = tokens.split()\n",
    "            embeddings = [glove_dict.get(word, np.zeros(embed_dim)) for word in words]\n",
    "            glove_reviews.append(torch.tensor(embeddings, dtype=torch.float))\n",
    "    else:\n",
    "        for review in tqdm(data, desc=f\"Retrieving {set} GloVe Word Embeddings\"):\n",
    "            words = review.split()\n",
    "            embeddings = [glove_dict.get(word, np.zeros(embed_dim)) for word in words]\n",
    "            glove_reviews.append(torch.tensor(embeddings, dtype=torch.float))\n",
    "\n",
    "    return pad_sequence(glove_reviews, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "Processing Reviews\n",
      "----------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building training GloVe dictionary: 100%|██████████| 36659/36659 [00:00<00:00, 264053.69it/s]\n",
      "Building validation GloVe dictionary: 100%|██████████| 13896/13896 [00:00<00:00, 657915.19it/s]\n",
      "Building test GloVe dictionary: 100%|██████████| 13971/13971 [00:00<00:00, 887521.71it/s]\n",
      "Retrieving training GloVe Word Embeddings: 100%|██████████| 1600/1600 [00:12<00:00, 128.18it/s]\n",
      "Retrieving validation GloVe Word Embeddings: 100%|██████████| 200/200 [00:01<00:00, 130.04it/s]\n",
      "Retrieving test GloVe Word Embeddings: 100%|██████████| 199/199 [00:01<00:00, 105.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "Processing Rationales\n",
      "----------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building training GloVe dictionary: 100%|██████████| 15318/15318 [00:00<00:00, 166617.00it/s]\n",
      "Building validation GloVe dictionary: 100%|██████████| 3335/3335 [00:00<00:00, 168112.92it/s]\n",
      "Building test GloVe dictionary: 100%|██████████| 1664/1664 [00:00<00:00, 100165.36it/s]\n",
      "Retrieving training GloVe Word Embeddings: 100%|██████████| 1600/1600 [00:02<00:00, 785.13it/s] \n",
      "Retrieving validation GloVe Word Embeddings: 100%|██████████| 200/200 [00:00<00:00, 2071.52it/s]\n",
      "Retrieving test GloVe Word Embeddings: 100%|██████████| 199/199 [00:00<00:00, 4038.84it/s]\n"
     ]
    }
   ],
   "source": [
    "print(f\"----------------------------------------------------------------------------------------\\nProcessing Reviews\\n----------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "# Extract vocabulary(distinct words) from training, validation, and test data\n",
    "train_vocab = set(word for review in train_df['content'] for word in review.split())\n",
    "val_vocab = set(word for review in val_df['content'] for word in review.split())\n",
    "test_vocab = set(word for review in test_df['content'] for word in review.split())\n",
    "\n",
    "# Build the GloVe dictionary for the reviews\n",
    "glove_dict = create_glove_dict(train_vocab, wv, \"training\")\n",
    "glove_dict.update(create_glove_dict(val_vocab, wv, \"validation\"))\n",
    "glove_dict.update(create_glove_dict(test_vocab, wv, \"test\"))\n",
    "\n",
    "# Convert reviews to glove embeddings\n",
    "train_review_gloves = get_w2GloVe(train_df['content'], glove_dict, \"training\")\n",
    "val_review_gloves = get_w2GloVe(val_df['content'], glove_dict, \"validation\")\n",
    "test_review_gloves = get_w2GloVe(test_df['content'], glove_dict, \"test\")\n",
    "\n",
    "print(f\"----------------------------------------------------------------------------------------\\nProcessing Rationales\\n----------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "#Extract vocabulary(distinct words) from training, validation, and test data for the rationales\n",
    "train_rationale_vocab = set(word for rationale in train_rationales for word in rationale.split())\n",
    "val_rationale_vocab = set(word for rationale in val_rationales for word in rationale.split())\n",
    "test_rationale_vocab = set(word for rationale in test_rationales for word in rationale.split())\n",
    "\n",
    "# Build the GloVe dictionary for the rationales\n",
    "dict_rat = create_glove_dict(train_rationale_vocab, wv, \"training\")\n",
    "dict_rat.update(create_glove_dict(val_rationale_vocab, wv, \"validation\"))\n",
    "dict_rat.update(create_glove_dict(test_rationale_vocab, wv, \"test\"))\n",
    "\n",
    "# Convert rationales to glove embeddings\n",
    "train_rationale_gloves = get_w2GloVe(train_rationales, glove_dict, \"training\", rationale=True)\n",
    "val_rationale_gloves = get_w2GloVe(val_rationales, glove_dict, \"validation\", rationale=True)\n",
    "test_rationale_gloves = get_w2GloVe(test_rationales, glove_dict, \"test\", rationale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the GloVe embeddings to local files for faster Access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "Pm21m4EZwz1F"
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"train_reviews.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_review_gloves, f)\n",
    "\n",
    "with open(\"val_reviews.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_review_gloves, f)\n",
    "\n",
    "with open(\"test_reviews.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_review_gloves, f)\n",
    "\n",
    "with open(\"train_rationales.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_rationale_gloves, f)\n",
    "\n",
    "with open(\"val_rationales.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_rationale_gloves, f)\n",
    "\n",
    "with open(\"test_rationales.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_rationale_gloves, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the GloVe embeddings created above and a create a copy before batching them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_reviews.pkl\", \"rb\") as f:\n",
    "    train_in = pickle.load(f)\n",
    "\n",
    "with open(\"train_rationales.pkl\", \"rb\") as f:\n",
    "    train_ev = pickle.load(f)\n",
    "\n",
    "with open(\"val_reviews.pkl\", \"rb\") as f:\n",
    "    val_in = pickle.load(f)\n",
    "\n",
    "with open(\"val_rationales.pkl\", \"rb\") as f:\n",
    "    val_ev = pickle.load(f)\n",
    "\n",
    "with open(\"test_reviews.pkl\", \"rb\") as f:\n",
    "    test_in = pickle.load(f)\n",
    "\n",
    "with open(\"test_rationales.pkl\", \"rb\") as f:\n",
    "    test_ev = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSzDGF43ytnN"
   },
   "source": [
    "Convert the training, validation, and test data(GloVe representations) including the rationales to batches using DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "wsgVwakDytnO"
   },
   "outputs": [],
   "source": [
    "train_inputs = TensorDataset(train_in, train_ev, train_classes)\n",
    "val_inputs = TensorDataset(val_in, val_ev, val_classes)\n",
    "test_inputs = TensorDataset(test_in, test_ev, test_classes)\n",
    "\n",
    "train_loader = DataLoader(train_inputs, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_inputs, batch_size=25, shuffle=False)\n",
    "test_loader = DataLoader(test_inputs, batch_size=25, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPQy0BjBytnO"
   },
   "source": [
    "# Convolutional Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "id": "smXuMplUytnO"
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "\n",
    "class LIME_CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        cnn_config: List[Dict],\n",
    "    ) -> None:\n",
    "        \n",
    "        super(LIME_CNN, self).__init__()\n",
    "        \n",
    "        self.in_channels = embed_dim\n",
    "        \n",
    "        self.config = cnn_config\n",
    "\n",
    "        self.cnn_layers = nn.ModuleList()\n",
    "\n",
    "        for index, config in enumerate(cnn_config):\n",
    "                            \n",
    "            conv_layer = nn.Conv1d(\n",
    "                in_channels=self.in_channels,\n",
    "                out_channels=config['out_channels'],\n",
    "                kernel_size=config['kernel_size'],\n",
    "                stride=config['stride'],\n",
    "                padding=config['padding'],\n",
    "                bias=config['bias'],\n",
    "            )\n",
    "            self.cnn_layers.append(conv_layer)\n",
    "\n",
    "            if index == 0 or index == len(cnn_config) - 1:\n",
    "                \n",
    "                maxPool_layer = nn.MaxPool1d(\n",
    "                    kernel_size=config['kernel_size'],\n",
    "                    stride=config['stride'],\n",
    "                    padding=config['padding'],\n",
    "                )\n",
    "                self.cnn_layers.append(maxPool_layer)    \n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        X: Tensor,\n",
    "    ) -> Tensor:\n",
    "        \n",
    "        X = X.permute(0, 2, 1)\n",
    "        for layer in self.cnn_layers:\n",
    "\n",
    "            conv_out = self.relu(layer(X)) if isinstance(layer, nn.Conv1d) else layer(X)\n",
    "\n",
    "        X = X.mean(dim=-1)\n",
    "        \n",
    "        self.fc_layer = nn.Linear(X.shape[1], 1)\n",
    "\n",
    "        y_hat = self.fc_layer(X)\n",
    "       \n",
    "        y_hat = torch.sigmoid(y_hat).squeeze(-1)\n",
    "        \n",
    "        return y_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOONmTg-ytnP"
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "0IXthmm0ytnP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 50/50 [00:07<00:00,  6.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6942\n",
      "Validation Loss: 0.7029, Accuracy: 0.4200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 50/50 [00:11<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6949\n",
      "Validation Loss: 0.6792, Accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 50/50 [00:10<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6922\n",
      "Validation Loss: 0.7356, Accuracy: 0.1450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 50/50 [00:10<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6957\n",
      "Validation Loss: 0.7109, Accuracy: 0.3100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 50/50 [00:10<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6957\n",
      "Validation Loss: 0.7282, Accuracy: 0.3800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 50/50 [00:12<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6955\n",
      "Validation Loss: 0.6914, Accuracy: 0.6700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 50/50 [00:11<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6966\n",
      "Validation Loss: 0.6939, Accuracy: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 50/50 [00:10<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6925\n",
      "Validation Loss: 0.7053, Accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 50/50 [00:10<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6942\n",
      "Validation Loss: 0.7223, Accuracy: 0.3350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 50/50 [00:10<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6962\n",
      "Validation Loss: 0.7552, Accuracy: 0.3750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 50/50 [00:12<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6945\n",
      "Validation Loss: 0.7478, Accuracy: 0.3050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 50/50 [00:12<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6939\n",
      "Validation Loss: 0.7194, Accuracy: 0.3700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████| 50/50 [00:10<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6950\n",
      "Validation Loss: 0.6734, Accuracy: 0.7550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|██████████| 50/50 [00:12<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6956\n",
      "Validation Loss: 0.7293, Accuracy: 0.3950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|██████████| 50/50 [00:11<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6967\n",
      "Validation Loss: 0.7346, Accuracy: 0.4100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|██████████| 50/50 [00:10<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6948\n",
      "Validation Loss: 0.6873, Accuracy: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: 100%|██████████| 50/50 [00:10<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6952\n",
      "Validation Loss: 0.7042, Accuracy: 0.4150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: 100%|██████████| 50/50 [00:10<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6949\n",
      "Validation Loss: 0.7312, Accuracy: 0.2700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|██████████| 50/50 [00:11<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6968\n",
      "Validation Loss: 0.6915, Accuracy: 0.4450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: 100%|██████████| 50/50 [00:12<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6952\n",
      "Validation Loss: 0.6615, Accuracy: 0.6350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: 100%|██████████| 50/50 [00:13<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6973\n",
      "Validation Loss: 0.6705, Accuracy: 0.6150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|██████████| 50/50 [00:12<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6928\n",
      "Validation Loss: 0.6996, Accuracy: 0.4150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100: 100%|██████████| 50/50 [00:11<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6949\n",
      "Validation Loss: 0.7043, Accuracy: 0.5900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: 100%|██████████| 50/50 [00:09<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6937\n",
      "Validation Loss: 0.7306, Accuracy: 0.2650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: 100%|██████████| 50/50 [00:09<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6945\n",
      "Validation Loss: 0.7398, Accuracy: 0.2450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: 100%|██████████| 50/50 [00:10<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6954\n",
      "Validation Loss: 0.6560, Accuracy: 0.5750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: 100%|██████████| 50/50 [00:09<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6966\n",
      "Validation Loss: 0.6676, Accuracy: 0.6850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100:  10%|█         | 5/50 [00:01<00:11,  4.09it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[202], line 61\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions, train_loss, loss_list\n\u001b[0;32m     59\u001b[0m model \u001b[38;5;241m=\u001b[39m LIME_CNN(embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, cnn_config\u001b[38;5;241m=\u001b[39mloaded_config)\n\u001b[1;32m---> 61\u001b[0m train_model(\n\u001b[0;32m     62\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     63\u001b[0m     train_set\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m     64\u001b[0m     val_set\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[0;32m     65\u001b[0m     n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     66\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[0;32m     67\u001b[0m )\n",
      "Cell \u001b[1;32mIn[202], line 22\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_set, val_set, n_epochs, lr)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     21\u001b[0m     inputs, rationales, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m---> 22\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     23\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(pred)\n\u001b[0;32m     24\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(pred, labels)\n",
      "File \u001b[1;32mc:\\Users\\kadap\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kadap\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[199], line 51\u001b[0m, in \u001b[0;36mLIME_CNN.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     48\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn_layers:\n\u001b[1;32m---> 51\u001b[0m     conv_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(layer(X)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, nn\u001b[38;5;241m.\u001b[39mConv1d) \u001b[38;5;28;01melse\u001b[39;00m layer(X)\n\u001b[0;32m     53\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kadap\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kadap\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\kadap\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:133\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[1;32mc:\\Users\\kadap\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1704\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Load the model configuration from a JSON file\n",
    "with open(\"model_config.json\", \"r\") as f:\n",
    "    loaded_config = json.load(f)\n",
    "\n",
    "def train_model(model, train_set, val_set, n_epochs, lr):\n",
    "\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_loss = float('inf') # initialize the best loss the model can achieve to infinity\n",
    "    patience = 0 # initialize the patience for early stopping if validation loss plateaus\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        avg_loss = 0.0\n",
    "        predictions = []\n",
    "        loss_list = []\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\"):\n",
    "            inputs, rationales, labels = batch\n",
    "            pred = model(inputs)\n",
    "            predictions.append(pred)\n",
    "            loss = criterion(pred, labels)\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        loss_list.append(train_loss)\n",
    "        print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        tp = 0\n",
    "        num_labels = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs, rationales, labels = batch\n",
    "                pred = model(inputs)\n",
    "                loss = criterion(pred, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                y_hats = (pred > 0.5).float()\n",
    "                tp += torch.sum(y_hats == labels).item()\n",
    "                num_labels += labels.size(0)\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = tp / num_labels\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\\n\")\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_model_state.pt\")\n",
    "\n",
    "    return predictions, train_loss, loss_list\n",
    "\n",
    "model = LIME_CNN(embed_dim=50, cnn_config=loaded_config)\n",
    "\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_set=train_loader,\n",
    "    val_set=val_loader,\n",
    "    n_epochs=100,\n",
    "    lr=0.001,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
