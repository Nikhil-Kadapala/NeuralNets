{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6L2Do3b_ytm6"
   },
   "source": [
    "# Final Project CS852 - Foundations of Neural Networks (FALL 2024)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICkcYT3Iytm9"
   },
   "source": [
    "Devin Borchard and Nikhil Kadapala\n",
    "Department of Computer Science, University of New Hampshire\n",
    "\n",
    "ERASER datasets: https://www.eraserbenchmark.com/\n",
    "\n",
    "ERASER paper: https://arxiv.org/pdf/1911.03429\n",
    "\n",
    "LIME paper: https://arxiv.org/pdf/1602.04938"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sr8uEiG3ytm9"
   },
   "source": [
    "# Notebook setup and PyTorch Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "LJ5b9esmytm-"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# uncomment one of these versions (depending on whether you are on a computer with a CPU or not)\n",
    "\n",
    "# GPU version\n",
    "# !conda install --yes --prefix {sys.prefix} pytorch torchvision cudatoolkit=10.2 -c pytorch\n",
    "\n",
    "# Just CPU\n",
    "# !conda install --yes --prefix {sys.prefix} pytorch torchvision cpuonly -c pytorch\n",
    "\n",
    "# install `Einops` for einstein-style tensor manipulation in pytorch\n",
    "# Also see https://github.com/arogozhnikov/einops\n",
    "# !conda install --yes --prefix {sys.prefix} einops  -c conda-forge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XSKM3pNDytnA",
    "outputId": "85479ac4-1c3d-48d0-8e6d-b5423f34fef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8461, 0.6464, 0.6018],\n",
      "        [0.8275, 0.2756, 0.4281],\n",
      "        [0.2685, 0.9006, 0.6929],\n",
      "        [0.3972, 0.3797, 0.8757],\n",
      "        [0.6981, 0.7389, 0.8229]])\n",
      "GPU/CUDA available?  False\n",
      "Torch version 2.4.1\n"
     ]
    }
   ],
   "source": [
    "# torch test\n",
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "\n",
    "print(\"GPU/CUDA available? \", torch.cuda.is_available())\n",
    "\n",
    "print(\"Torch version\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIw0tI1IytnB"
   },
   "source": [
    "# Parse the data files to extract the reviews, classifications and annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIrDTApeytnC"
   },
   "source": [
    "There are two files:\n",
    "- train.jsonl: containts 1600 training examples\n",
    "- test.json: contains 199 test examples\n",
    "\n",
    "Each example includes:\n",
    "- annotation_id: a unique id for an example of the form negR_000 for negative examples and posR_000 for positive examples.\n",
    "- evidences: a list of rationales(specific parts of the review) given by humans that most influenced their classification decision.\n",
    "- classification: the class of the example\n",
    "\n",
    "The annotation_id of each example is the name of the file for the input text data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "pVM3qH6FytnD",
    "outputId": "1c58a3c2-1499-4754-99ea-f5c1596fc767"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_data(file_path):\n",
    "    data = []                                               # Initialize an empty list to store the dictionaries\n",
    "                                \n",
    "    with open(file_path, 'r') as file:                      # Open the .jsonl file and read it line by line\n",
    "        for line in file:\n",
    "            annotation = json.loads(line)                   # Parse each line as JSON and append it to the list\n",
    "            id = annotation[\"annotation_id\"]\n",
    "            annotation[\"classification\"] = 1 if annotation['classification'] == \"POS\" else 0\n",
    "            \n",
    "            with open(f\"./movies/docs/{id}\", 'r') as file:  # open the file named by annotation_id to extract the review text\n",
    "                content = file.read()\n",
    "                annotation['content'] = content.replace('\\n', ' ')\n",
    "                data.append(annotation)\n",
    "    return data\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "train_file_path = './movies/train.jsonl'\n",
    "test_file_path = './movies/test.jsonl'\n",
    "\n",
    "train_data = parse_data(train_file_path)\n",
    "test_data = parse_data(test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bijy4N-HytnD"
   },
   "source": [
    "# Define Functions to extract reviews, lassifications, and annotations\n",
    "  Define a function to retrieve an example and print the relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKGZxYzTytnE",
    "outputId": "e9382a49-774e-4967-cbf0-895da5006e49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: 1600 training examples\n",
      "               199 test examples\n",
      "\n",
      "Retrieving Training Example [506].................\n",
      "\n",
      "Review content:\n",
      "this film is extraordinarily horrendous and i 'm not going to waste any more words on it .\n",
      "\n",
      "---------------------------- \n",
      "| Sentiment class: 0 - NEG | \n",
      "----------------------------\n",
      "\n",
      "Human rationales / Supporting Evidence:\n",
      "     -  extraordinarily horrendous\n"
     ]
    }
   ],
   "source": [
    "def print_example(data, index, print_content=True, print_classification=True, print_rationales=True ):\n",
    "    print(f'Retrieving Training Example [{index}].................\\n')\n",
    "    item = data[index]\n",
    "    classification = item['classification']\n",
    "    evidences = item['evidences']\n",
    "    content = item['content']\n",
    "    if print_content: print(f'Review content:\\n{content}\\n')\n",
    "    if print_classification: print('----------------------------',\n",
    "                                   '\\n| Sentiment class:', \n",
    "                                   classification, \n",
    "                                   (\"- NEG\" if not classification else \"- POS\"), \n",
    "                                   '|', '\\n----------------------------')\n",
    "    if print_rationales:\n",
    "        print('\\nHuman rationales / Supporting Evidence:')\n",
    "        for evidence in evidences:\n",
    "            print('     - ', evidence[0]['text'])\n",
    "\n",
    "def get_content(data, index):\n",
    "    item = data[index]\n",
    "    content = item['content']\n",
    "    return content\n",
    "\n",
    "def get_classes(data, index):\n",
    "    item = data[index]\n",
    "    classification = item['classification']\n",
    "    return classification\n",
    "\n",
    "def get_annotations(data, index):\n",
    "    item = data[index]\n",
    "    content = item['evidences']\n",
    "    annotations = [evidence[0]['text'] for evidence in content]\n",
    "    return annotations\n",
    "\n",
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "\n",
    "print(f'Dataset split: {train_size} training examples')\n",
    "print(f'               {test_size} test examples\\n')\n",
    "\n",
    "print_example(train_data, 506)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the human rationales from the evidences metadata of each review example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['extraordinarily horrendous']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_data)):\n",
    "    train_data[i]['evidences'] = get_annotations(train_data,i)\n",
    "\n",
    "print(train_data[506]['evidences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vc2rC9QTytnK"
   },
   "source": [
    "# Pre-trianed GloVe Embeddings of Training Examples\n",
    "Download the pretrained GloVe Embeddings of desired dimensions using gensim downlader.\n",
    "\n",
    "Save downloaded embeddings to a local file to avoid re-downloading when the kernel or notebook is restarted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWyAJ5D_ytnL",
    "outputId": "355ef868-fcb6-4d80-efdf-5514662c6235"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Install gensim, to use word2vec word embeddings\n",
    "    Install gensim (for pre-trained word embeddings)\n",
    "    #!conda install --yes --prefix {sys.prefix} gensim\n",
    "\"\"\"\n",
    "#import gensim\n",
    "#import gensim.downloader\n",
    "\n",
    "\"\"\"\n",
    "    ONLY if you get an error after `import gensim`: update your smart_open liberary\n",
    "    #!conda install --yes --prefix {sys.prefix} smart_open\n",
    "    restart your notebook\n",
    "    see if `import gensim` works now\n",
    "\"\"\"\n",
    "#wv = gensim.downloader.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "#import pickle\n",
    "\n",
    "#with open(\"glove_embeddings.pkl\", \"wb\") as f:\n",
    "    #pickle.dump(wv, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.20356 , -0.8707  , -0.19172 ,  0.73862 ,  0.18494 ,  0.14926 ,\n",
       "        0.48079 , -0.21633 ,  0.72753 , -0.36912 ,  0.13397 , -0.1143  ,\n",
       "       -0.18075 , -0.64683 , -0.18484 ,  0.83575 ,  0.48179 ,  0.76026 ,\n",
       "       -0.50381 ,  0.80743 ,  1.2195  ,  0.3459  ,  0.22185 ,  0.31335 ,\n",
       "        1.2066  , -1.8441  ,  0.14064 , -0.99715 , -1.1402  ,  0.32342 ,\n",
       "        3.2128  ,  0.42708 ,  0.19504 ,  0.80113 ,  0.38555 , -0.12568 ,\n",
       "       -0.26533 ,  0.055264, -1.1557  ,  0.16836 , -0.82228 ,  0.20394 ,\n",
       "        0.089235, -0.60125 , -0.032878,  1.3735  , -0.51661 ,  0.29611 ,\n",
       "        0.23951 , -1.3801  ], dtype=float32)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"glove_embeddings.pkl\", \"rb\") as f:\n",
    "    wv = pickle.load(f)\n",
    "    \n",
    "# lookup the word vector for a word \"india\"\n",
    "wv['india']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IFRN-vp2ytnM",
    "outputId": "35cf6fed-771d-4b6f-8fa6-16877054b9fb"
   },
   "outputs": [],
   "source": [
    "# downsampled embedding and zero vector for unknown words\n",
    "# note the following code assums the the word embedding dimensions are dividible by 5\n",
    "\n",
    "import einops # type: ignore\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import types\n",
    "\n",
    "def glove_embed(word:str, target_dim)->np.array:\n",
    "    '''Looks up word in embedding (downsampled to five dimensions), pads with beginning of embedding.\n",
    "       Returns zero vector for unknown words.\n",
    "    '''\n",
    "    # these parameters work for 50-dim glove embeddings (adjust for other embeddings)\n",
    "    sampled_dim = 5\n",
    "    sample_batches = 10\n",
    "\n",
    "    empty_vec=np.zeros(target_dim)\n",
    "    if word in wv:\n",
    "        w2v = wv[word] # lookup 50 dim vector\n",
    "        a=einops.reduce(w2v,'(d seg)-> d', \"sum\", seg=sample_batches)  # downsample\n",
    "        b=w2v[0:target_dim-sampled_dim]\n",
    "        return np.hstack([a,b])\n",
    "    else:\n",
    "        return empty_vec\n",
    "\n",
    "def glove_embed_sequences(sequence, target_dim):\n",
    "    \n",
    "    if isinstance(sequence, list):\n",
    "        if len(sequence) == 0:\n",
    "            empty_seq = np.zeros(target_dim)\n",
    "            gloveTensor =  torch.tensor(empty_seq, dtype=torch.float)\n",
    "        else:\n",
    "            tokens = \",\".join(sequence)\n",
    "            words = tokens.split()\n",
    "            gloveTensor = torch.stack([torch.tensor(glove_embed(word, target_dim), dtype=torch.float) for word in words])\n",
    "    else: \n",
    "        tokens = sequence.split()\n",
    "        gloveTensor = torch.stack([torch.tensor(glove_embed(token, target_dim), dtype=torch.float) for token in tokens])\n",
    "    \n",
    "    return gloveTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XoZiHbVxytnN",
    "outputId": "29b22230-51ff-4fe1-b918-c39e899a3839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews in training data: 1600\n",
      "Max seq length of reviews: 2809\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_id</th>\n",
       "      <th>classification</th>\n",
       "      <th>evidences</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negR_000.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>[mind - fuck movie, the sad part is, downshift...</td>\n",
       "      <td>plot : two teen couples go to a church party ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negR_001.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>[it 's pretty much a sunken ship, sutherland i...</td>\n",
       "      <td>the happy bastard 's quick movie review damn t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negR_002.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>[the characters and acting is nothing spectacu...</td>\n",
       "      <td>it is movies like these that make a jaded movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negR_003.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>[dead on arrival, the characters stink, subpar...</td>\n",
       "      <td>\" quest for camelot \" is warner bros . ' first...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negR_004.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>[it is highly derivative and somewhat boring, ...</td>\n",
       "      <td>synopsis : a mentally unstable man undergoing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotation_id  classification  \\\n",
       "0  negR_000.txt               0   \n",
       "1  negR_001.txt               0   \n",
       "2  negR_002.txt               0   \n",
       "3  negR_003.txt               0   \n",
       "4  negR_004.txt               0   \n",
       "\n",
       "                                           evidences  \\\n",
       "0  [mind - fuck movie, the sad part is, downshift...   \n",
       "1  [it 's pretty much a sunken ship, sutherland i...   \n",
       "2  [the characters and acting is nothing spectacu...   \n",
       "3  [dead on arrival, the characters stink, subpar...   \n",
       "4  [it is highly derivative and somewhat boring, ...   \n",
       "\n",
       "                                             content  \n",
       "0  plot : two teen couples go to a church party ,...  \n",
       "1  the happy bastard 's quick movie review damn t...  \n",
       "2  it is movies like these that make a jaded movi...  \n",
       "3  \" quest for camelot \" is warner bros . ' first...  \n",
       "4  synopsis : a mentally unstable man undergoing ...  "
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reviews = [get_content(train_data, i) for i in range(train_size)]\n",
    "classes = [get_classes(train_data, i) for i in range(train_size)]\n",
    "rationales = [get_annotations(train_data, i) for i in range(train_size)]\n",
    "\n",
    "print(\"Number of reviews in training data:\",len(reviews))\n",
    "print(\"Max seq length of reviews:\", np.max([len(review.split()) for review in reviews]))\n",
    "\n",
    "df = pd.DataFrame(train_data)\n",
    "df.drop(columns=['query', 'query_type'], inplace=True)\n",
    "df['evidences'] = rationales\n",
    "df.head()\n",
    "#rationales[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation_id                                          negR_506.txt\n",
      "classification                                                    0\n",
      "evidences                              [extraordinarily horrendous]\n",
      "content           this film is extraordinarily horrendous and i ...\n",
      "Name: 506, dtype: object\n",
      "this film is extraordinarily horrendous and i 'm not going to waste any more words on it . \n",
      "0 \n",
      "['extraordinarily horrendous']\n"
     ]
    }
   ],
   "source": [
    "# Find the smallest review of all to inspect and understand the training data structure.\n",
    "smallest_entry = df.loc[df['content'].apply(len).idxmin()]\n",
    "print(smallest_entry)\n",
    "id = 'negR_506.txt'\n",
    "index = df['content'].apply(len).idxmin()\n",
    "print(f'{df.iloc[506]['content']} \\n{df.iloc[506]['classification']} \\n{df.iloc[506]['evidences']}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract validation set from the val.jsonl file and create a dataframe for it similar to the training set and save it to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the reviews to their corresponding Glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "reviewGloVes = [glove_embed_sequences(review, 11) for review in df['content']]\n",
    "reviewGloVes = pad_sequence(reviewGloVes, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the rationales to their corresponding Glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rationaleGloVes = [glove_embed_sequences(rationale, 11) for rationale in rationales]\n",
    "rationaleGloVes = pad_sequence(reviewGloVes, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSzDGF43ytnN"
   },
   "source": [
    "# Convert the training data to batches using DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wsgVwakDytnO",
    "outputId": "b91c8682-3b9f-4a8c-8cb6-ee47f1599519"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dim (batches, max_seq_len, embed_size): torch.Size([1600, 2809, 11])\n"
     ]
    }
   ],
   "source": [
    "input = reviewGloVes\n",
    "print(\"input dim (batches, max_seq_len, embed_size):\",input.size())\n",
    "Y_star = torch.tensor(classes, dtype=torch.float)\n",
    "train_x = TensorDataset(input, Y_star)\n",
    "dataLoader = DataLoader(train_x, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPQy0BjBytnO"
   },
   "source": [
    "# Convolutional Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "smXuMplUytnO"
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "from torch import Tensor\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Tuple[int, ...],\n",
    "        pool_size: Tuple[int, ...],\n",
    "        stride: Tuple[int, ...],\n",
    "        padding: Tuple[int, ...],\n",
    "        bias: bool\n",
    "    ) -> None:\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "\n",
    "        super(MyModel, self).__init__()\n",
    "        self.ReLU_Activation = nn.ReLU()\n",
    "        self.convolutionLayer = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.poolingLayer = nn.MaxPool1d(kernel_size=pool_size, stride=stride, padding=padding)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        X: Tensor,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Embedding Layer X --Theta--> E\n",
    "        Input: X (Tensor)\n",
    "        Output: E (Tensor)\n",
    "        Parameter: Theta (optional)\n",
    "        \"\"\"\n",
    "\n",
    "        X = X.squeeze(0)\n",
    "        X_shape = list(X.size())\n",
    "        #print(X_shape)\n",
    "        #print(X)\n",
    "        self.embeddingLayer = nn.Linear(2809, self.in_channels, bias=False)\n",
    "        self.embeddingLayer.weight.data = torch.randn((3,11),dtype=torch.float)\n",
    "        E = self.embeddingLayer(X)\n",
    "        #print(E)\n",
    "\n",
    "        E_shape = list(E.size())\n",
    "        if len(E_shape) < 3:\n",
    "            E = E.unsqueeze(0)\n",
    "        E = E.permute(0, 2, 1)\n",
    "\n",
    "        \"\"\"\n",
    "        Convolution Layer E --W--> H\n",
    "        Input: E (Tensor)\n",
    "        Output: H (Tensor)\n",
    "        Parameter: W (optional)\n",
    "        \"\"\"\n",
    "        #self.convolutionLayer.weight.data = W_torch.permute(2, 0, 1)\n",
    "        H = self.convolutionLayer(E)\n",
    "        #print(H)\n",
    "        H = H.squeeze(0)\n",
    "        H = H.permute(1, 0)\n",
    "\n",
    "        \"\"\"\n",
    "        Dectector Layer H --Psi--> D with ReLU\n",
    "        Input: H (Tensor)\n",
    "        Output: D (Tensor)\n",
    "        Parameter: Psi (optional)\n",
    "        \"\"\"\n",
    "        myPsi_torch = torch.randn((2,1), dtype=torch.float)\n",
    "        D = torch.einsum('ij,jk->ik', H, myPsi_torch)\n",
    "        self.ReLU_Activation = nn.ReLU()\n",
    "        D = self.ReLU_Activation(D)\n",
    "        #print(D)\n",
    "        D = D.permute(1, 0)\n",
    "\n",
    "        \"\"\"\n",
    "        Pooling Layer D --MaxPool--> Y\n",
    "        Input: D (Tensor)\n",
    "        output: Y (Tensor)\n",
    "        Parameter: Pooling_window (optional)\n",
    "        \"\"\"\n",
    "        P = self.poolingLayer(D)\n",
    "        Y_hat = torch.zeros(X_shape[0])\n",
    "        D_shape = list(D.size())\n",
    "        out_size =  D_shape[1] - self.pool_size // self.stride + 1\n",
    "        Y_hat[:out_size] = P\n",
    "\n",
    "        return Y_hat\n",
    "\n",
    "model = MyModel(3, 2, 3, 2, 1, 0, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOONmTg-ytnP"
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0IXthmm0ytnP",
    "outputId": "fe01cf0d-5d91-4174-c832-4e30b56beeb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "Training batch: 1\n",
      "-----------------------------------------------------------------------\n",
      "epoch:  40  loss:  0.23346801102161407\n",
      "epoch:  80  loss:  0.022882001474499702\n",
      "epoch:  120  loss:  0.29552361369132996\n",
      "epoch:  160  loss:  0.07584503293037415\n",
      "epoch:  200  loss:  0.2678920030593872\n",
      "-----------------------------------------------------------------------\n",
      "Training batch: 2\n",
      "-----------------------------------------------------------------------\n",
      "epoch:  40  loss:  0.12465197592973709\n",
      "epoch:  80  loss:  0.0015520385932177305\n",
      "epoch:  120  loss:  0.03889119625091553\n",
      "epoch:  160  loss:  0.0011279884492978454\n",
      "epoch:  200  loss:  0.012324703857302666\n",
      "-----------------------------------------------------------------------\n",
      "Training batch: 3\n",
      "-----------------------------------------------------------------------\n",
      "epoch:  40  loss:  0.019592268392443657\n",
      "epoch:  80  loss:  0.007687563542276621\n",
      "epoch:  120  loss:  0.015764998272061348\n",
      "epoch:  160  loss:  0.012404659762978554\n",
      "epoch:  200  loss:  0.0018595827277749777\n",
      "-----------------------------------------------------------------------\n",
      "Training batch: 4\n",
      "-----------------------------------------------------------------------\n",
      "epoch:  40  loss:  6.462549208663404e-05\n",
      "epoch:  80  loss:  0.003729760879650712\n",
      "epoch:  120  loss:  0.008672282099723816\n",
      "epoch:  160  loss:  0.002799536567181349\n",
      "epoch:  200  loss:  0.00031126977410167456\n",
      "-----------------------------------------------------------------------\n",
      "Training batch: 5\n",
      "-----------------------------------------------------------------------\n",
      "epoch:  40  loss:  0.0002798825444187969\n",
      "epoch:  80  loss:  1.2413206604833249e-05\n",
      "epoch:  120  loss:  9.048546417034231e-06\n",
      "epoch:  160  loss:  5.147125648363726e-06\n",
      "epoch:  200  loss:  1.4669426491309423e-05\n",
      "======================================================================\n",
      "MSE for ground truth y_star tensor(0.2825, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def train(xdata, ydata):\n",
    "    '''Train the neural model with the given training data'''\n",
    "\n",
    "    #Construct the loss function\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    # Construct the optimizer (Stochastic Gradient Descent in this case)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)  # lr is learning rate\n",
    "\n",
    "    # Gradient Descent\n",
    "    for epoch in range(201):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        Y_pred = model(xdata)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(Y_pred, ydata)\n",
    "\n",
    "        if epoch >0 and epoch % 40 == 0:\n",
    "            print('epoch: ', epoch,' loss: ', loss.item())\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # perform a backward pass (backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    return Y_pred\n",
    "\n",
    "batch_input = torch.zeros((2809, 11), dtype=torch.float)\n",
    "predList = []\n",
    "batch_outputs = torch.zeros(5, dtype=torch.float)\n",
    "k = 0\n",
    "for batch in dataLoader:\n",
    "    batch_input = batch[0]\n",
    "    y_star = Y_star[k]\n",
    "    k += 1\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    print(\"Training batch:\", k)\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    Y_pred = train(batch_input, y_star)\n",
    "    #print(Y_pred.size())\n",
    "    predList.append(Y_pred[:5])\n",
    "    if k == 5:\n",
    "        break\n",
    "batch_outputs = torch.stack(predList)\n",
    "#print(batch_outputs.size())\n",
    "print(\"======================================================================\")\n",
    "mse=torch.mean( (batch_outputs-Y_star[:5])**2)  # detach takes the tensor out of the network\n",
    "print(\"MSE for ground truth y_star\", mse)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
